  duration_time                                      [Tool event]
  user_time                                          [Tool event]
  system_time                                        [Tool event]
  branch-instructions OR cpu/branch-instructions/    [Kernel PMU event]
  branch-misses OR cpu/branch-misses/                [Kernel PMU event]
  bus-cycles OR cpu/bus-cycles/                      [Kernel PMU event]
  cache-misses OR cpu/cache-misses/                  [Kernel PMU event]
  cache-references OR cpu/cache-references/          [Kernel PMU event]
  cpu-cycles OR cpu/cpu-cycles/                      [Kernel PMU event]
  instructions OR cpu/instructions/                  [Kernel PMU event]
  mem-loads OR cpu/mem-loads/                        [Kernel PMU event]
  mem-stores OR cpu/mem-stores/                      [Kernel PMU event]
  ref-cycles OR cpu/ref-cycles/                      [Kernel PMU event]
  topdown-fetch-bubbles OR cpu/topdown-fetch-bubbles/[Kernel PMU event]
  topdown-recovery-bubbles OR cpu/topdown-recovery-bubbles/[Kernel PMU event]
  topdown-slots-issued OR cpu/topdown-slots-issued/  [Kernel PMU event]
  topdown-slots-retired OR cpu/topdown-slots-retired/[Kernel PMU event]
  topdown-total-slots OR cpu/topdown-total-slots/    [Kernel PMU event]
  cstate_core/c3-residency/                          [Kernel PMU event]
  cstate_core/c6-residency/                          [Kernel PMU event]
  cstate_core/c7-residency/                          [Kernel PMU event]
  cstate_pkg/c10-residency/                          [Kernel PMU event]
  cstate_pkg/c2-residency/                           [Kernel PMU event]
  cstate_pkg/c3-residency/                           [Kernel PMU event]
  cstate_pkg/c6-residency/                           [Kernel PMU event]
  cstate_pkg/c7-residency/                           [Kernel PMU event]
  cstate_pkg/c8-residency/                           [Kernel PMU event]
  cstate_pkg/c9-residency/                           [Kernel PMU event]
  i915/actual-frequency/                             [Kernel PMU event]
  i915/bcs0-busy/                                    [Kernel PMU event]
  i915/bcs0-sema/                                    [Kernel PMU event]
  i915/bcs0-wait/                                    [Kernel PMU event]
  i915/interrupts/                                   [Kernel PMU event]
  i915/rc6-residency/                                [Kernel PMU event]
  i915/rcs0-busy/                                    [Kernel PMU event]
  i915/rcs0-sema/                                    [Kernel PMU event]
  i915/rcs0-wait/                                    [Kernel PMU event]
  i915/requested-frequency/                          [Kernel PMU event]
  i915/software-gt-awake-time/                       [Kernel PMU event]
  i915/vcs0-busy/                                    [Kernel PMU event]
  i915/vcs0-sema/                                    [Kernel PMU event]
  i915/vcs0-wait/                                    [Kernel PMU event]
  i915/vecs0-busy/                                   [Kernel PMU event]
  i915/vecs0-sema/                                   [Kernel PMU event]
  i915/vecs0-wait/                                   [Kernel PMU event]
  intel_bts//                                        [Kernel PMU event]
  intel_pt//                                         [Kernel PMU event]
  msr/aperf/                                         [Kernel PMU event]
  msr/cpu_thermal_margin/                            [Kernel PMU event]
  msr/mperf/                                         [Kernel PMU event]
  msr/pperf/                                         [Kernel PMU event]
  msr/smi/                                           [Kernel PMU event]
  msr/tsc/                                           [Kernel PMU event]
  power/energy-cores/                                [Kernel PMU event]
  power/energy-gpu/                                  [Kernel PMU event]
  power/energy-pkg/                                  [Kernel PMU event]
  power/energy-psys/                                 [Kernel PMU event]
  power/energy-ram/                                  [Kernel PMU event]
  uncore_cbox_0/clockticks/                          [Kernel PMU event]
  uncore_cbox_1/clockticks/                          [Kernel PMU event]
  uncore_cbox_2/clockticks/                          [Kernel PMU event]
  uncore_cbox_3/clockticks/                          [Kernel PMU event]
  uncore_imc/data_reads/                             [Kernel PMU event]
  uncore_imc/data_writes/                            [Kernel PMU event]
  uncore_imc/gt_requests/                            [Kernel PMU event]
  uncore_imc/ia_requests/                            [Kernel PMU event]
  uncore_imc/io_requests/                            [Kernel PMU event]

cache:
  l1d.replacement
       [L1D data line replacements]
  l1d_pend_miss.fb_full
       [Number of times a request needed a FB entry but there was no entry
        available for it. That is the FB unavailability was dominant reason
        for blocking the request. A request includes cacheable/uncacheable
        demands that is load, store or SW prefetch]
  l1d_pend_miss.pending
       [L1D miss outstandings duration in cycles]
  l1d_pend_miss.pending_cycles
       [Cycles with L1D load Misses outstanding]
  l1d_pend_miss.pending_cycles_any
       [Cycles with L1D load Misses outstanding from any thread on physical
        core]
  l2_lines_in.all
       [L2 cache lines filling L2]
  l2_lines_out.non_silent
       [Counts the number of lines that are evicted by L2 cache when triggered
        by an L2 cache fill. Those lines are in Modified state. Modified lines
        are written back to L3]
  l2_lines_out.silent
       [Counts the number of lines that are silently dropped by L2 cache when
        triggered by an L2 cache fill. These lines are typically in Shared or
        Exclusive state. A non-threaded event]
  l2_lines_out.useless_hwpf
       [Counts the number of lines that have been hardware prefetched but not
        used and now evicted by L2 cache]
  l2_rqsts.all_code_rd
       [L2 code requests]
  l2_rqsts.all_demand_data_rd
       [Demand Data Read requests]
  l2_rqsts.all_demand_miss
       [Demand requests that miss L2 cache]
  l2_rqsts.all_demand_references
       [Demand requests to L2 cache]
  l2_rqsts.all_pf
       [Requests from the L1/L2/L3 hardware prefetchers or Load software
        prefetches]
  l2_rqsts.all_rfo
       [RFO requests to L2 cache]
  l2_rqsts.code_rd_hit
       [L2 cache hits when fetching instructions, code reads]
  l2_rqsts.code_rd_miss
       [L2 cache misses when fetching instructions]
  l2_rqsts.demand_data_rd_hit
       [Demand Data Read requests that hit L2 cache]
  l2_rqsts.demand_data_rd_miss
       [Demand Data Read miss L2, no rejects]
  l2_rqsts.miss
       [All requests that miss L2 cache]
  l2_rqsts.pf_hit
       [Requests from the L1/L2/L3 hardware prefetchers or Load software
        prefetches that hit L2 cache]
  l2_rqsts.pf_miss
       [Requests from the L1/L2/L3 hardware prefetchers or Load software
        prefetches that miss L2 cache]
  l2_rqsts.references
       [All L2 requests]
  l2_rqsts.rfo_hit
       [RFO requests that hit L2 cache]
  l2_rqsts.rfo_miss
       [RFO requests that miss L2 cache]
  l2_trans.l2_wb
       [L2 writebacks that access L2 cache]
  longest_lat_cache.miss
       [Core-originated cacheable demand requests missed L3 Spec update:
        SKL057]
  longest_lat_cache.reference
       [Core-originated cacheable demand requests that refer to L3 Spec
        update: SKL057]
  mem_inst_retired.all_loads
       [Retired load instructions Supports address when precise (Precise
        event)]
  mem_inst_retired.all_stores
       [Retired store instructions Supports address when precise (Precise
        event)]
  mem_inst_retired.any
       [All retired memory instructions Supports address when precise (Precise
        event)]
  mem_inst_retired.lock_loads
       [Retired load instructions with locked access Supports address when
        precise (Precise event)]
  mem_inst_retired.split_loads
       [Retired load instructions that split across a cacheline boundary
        Supports address when precise (Precise event)]
  mem_inst_retired.split_stores
       [Retired store instructions that split across a cacheline boundary
        Supports address when precise (Precise event)]
  mem_inst_retired.stlb_miss_loads
       [Retired load instructions that miss the STLB Supports address when
        precise (Precise event)]
  mem_inst_retired.stlb_miss_stores
       [Retired store instructions that miss the STLB Supports address when
        precise (Precise event)]
  mem_load_l3_hit_retired.xsnp_hit
       [Retired load instructions which data sources were L3 and cross-core
        snoop hits in on-pkg core cache Supports address when precise (Precise
        event)]
  mem_load_l3_hit_retired.xsnp_hitm
       [Retired load instructions which data sources were HitM responses from
        shared L3 Supports address when precise (Precise event)]
  mem_load_l3_hit_retired.xsnp_miss
       [Retired load instructions which data sources were L3 hit and
        cross-core snoop missed in on-pkg core cache Supports address when
        precise (Precise event)]
  mem_load_l3_hit_retired.xsnp_none
       [Retired load instructions which data sources were hits in L3 without
        snoops required Supports address when precise (Precise event)]
  mem_load_misc_retired.uc
       [Retired instructions with at least 1 uncacheable load or lock Supports
        address when precise (Precise event)]
  mem_load_retired.fb_hit
       [Retired load instructions which data sources were load missed L1 but
        hit FB due to preceding miss to the same cache line with data not
        ready Supports address when precise (Precise event)]
  mem_load_retired.l1_hit
       [Retired load instructions with L1 cache hits as data sources Supports
        address when precise (Precise event)]
  mem_load_retired.l1_miss
       [Retired load instructions missed L1 cache as data sources Supports
        address when precise (Precise event)]
  mem_load_retired.l2_hit
       [Retired load instructions with L2 cache hits as data sources Supports
        address when precise (Precise event)]
  mem_load_retired.l2_miss
       [Retired load instructions missed L2 cache as data sources Supports
        address when precise (Precise event)]
  mem_load_retired.l3_hit
       [Retired load instructions with L3 cache hits as data sources Supports
        address when precise (Precise event)]
  mem_load_retired.l3_miss
       [Retired load instructions missed L3 cache as data sources Supports
        address when precise (Precise event)]
  offcore_requests.all_data_rd
       [Demand and prefetch data reads]
  offcore_requests.all_requests
       [Any memory transaction that reached the SQ]
  offcore_requests.demand_code_rd
       [Cacheable and non-cacheable code read requests]
  offcore_requests.demand_data_rd
       [Demand Data Read requests sent to uncore]
  offcore_requests.demand_rfo
       [Demand RFO requests including regular RFOs, locks, ItoM]
  offcore_requests_buffer.sq_full
       [Offcore requests buffer cannot take more entries for this thread core]
  offcore_requests_outstanding.all_data_rd
       [Offcore outstanding cacheable Core Data Read transactions in
        SuperQueue (SQ), queue to uncore]
  offcore_requests_outstanding.cycles_with_data_rd
       [Cycles when offcore outstanding cacheable Core Data Read transactions
        are present in SuperQueue (SQ), queue to uncore]
  offcore_requests_outstanding.cycles_with_demand_code_rd
       [Cycles with offcore outstanding Code Reads transactions in the
        SuperQueue (SQ), queue to uncore]
  offcore_requests_outstanding.cycles_with_demand_data_rd
       [Cycles when offcore outstanding Demand Data Read transactions are
        present in SuperQueue (SQ), queue to uncore]
  offcore_requests_outstanding.cycles_with_demand_rfo
       [Cycles with offcore outstanding demand rfo reads transactions in
        SuperQueue (SQ), queue to uncore]
  offcore_requests_outstanding.demand_code_rd
       [Offcore outstanding Code Reads transactions in the SuperQueue (SQ),
        queue to uncore, every cycle]
  offcore_requests_outstanding.demand_data_rd
       [Offcore outstanding Demand Data Read transactions in uncore queue]
  offcore_requests_outstanding.demand_data_rd_ge_6
       [Cycles with at least 6 offcore outstanding Demand Data Read
        transactions in uncore queue]
  offcore_requests_outstanding.demand_rfo
       [Offcore outstanding demand rfo reads transactions in SuperQueue (SQ),
        queue to uncore, every cycle]
  offcore_response
       [Offcore response can be programmed only with a specific pair of event
        select and counter MSR, and with specific event codes and predefine
        mask bit value in a dedicated MSR to specify attributes of the offcore
        transaction]
  offcore_response.demand_code_rd.any_response
       [Counts all demand code reads have any response type]
  offcore_response.demand_code_rd.l3_hit.any_snoop
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_hit.snoop_hit_no_fwd
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_hit.snoop_hitm
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_hit.snoop_miss
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_hit.snoop_none
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_hit.snoop_not_needed
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_hit.spl_hit
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_hit_e.any_snoop
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_hit_e.snoop_hit_no_fwd
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_hit_e.snoop_hitm
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_hit_e.snoop_miss
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_hit_e.snoop_none
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_hit_e.snoop_not_needed
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_hit_e.spl_hit
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_hit_m.any_snoop
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_hit_m.snoop_hit_no_fwd
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_hit_m.snoop_hitm
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_hit_m.snoop_miss
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_hit_m.snoop_none
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_hit_m.snoop_not_needed
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_hit_m.spl_hit
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_hit_s.any_snoop
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_hit_s.snoop_hit_no_fwd
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_hit_s.snoop_hitm
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_hit_s.snoop_miss
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_hit_s.snoop_none
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_hit_s.snoop_not_needed
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_hit_s.spl_hit
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l4_hit_local_l4.any_snoop
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l4_hit_local_l4.snoop_hit_no_fwd
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l4_hit_local_l4.snoop_hitm
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l4_hit_local_l4.snoop_miss
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l4_hit_local_l4.snoop_none
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l4_hit_local_l4.snoop_not_needed
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l4_hit_local_l4.spl_hit
       [Counts all demand code reads]
  offcore_response.demand_code_rd.supplier_none.any_snoop
       [Counts all demand code reads]
  offcore_response.demand_code_rd.supplier_none.snoop_hit_no_fwd
       [Counts all demand code reads]
  offcore_response.demand_code_rd.supplier_none.snoop_hitm
       [Counts all demand code reads]
  offcore_response.demand_code_rd.supplier_none.snoop_miss
       [Counts all demand code reads]
  offcore_response.demand_code_rd.supplier_none.snoop_none
       [Counts all demand code reads]
  offcore_response.demand_code_rd.supplier_none.snoop_not_needed
       [Counts all demand code reads]
  offcore_response.demand_code_rd.supplier_none.spl_hit
       [Counts all demand code reads]
  offcore_response.demand_data_rd.any_response
       [Counts demand data reads have any response type]
  offcore_response.demand_data_rd.l3_hit.any_snoop
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_hit.snoop_hit_no_fwd
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_hit.snoop_hitm
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_hit.snoop_miss
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_hit.snoop_none
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_hit.snoop_not_needed
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_hit.spl_hit
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_hit_e.any_snoop
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_hit_e.snoop_hit_no_fwd
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_hit_e.snoop_hitm
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_hit_e.snoop_miss
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_hit_e.snoop_none
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_hit_e.snoop_not_needed
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_hit_e.spl_hit
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_hit_m.any_snoop
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_hit_m.snoop_hit_no_fwd
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_hit_m.snoop_hitm
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_hit_m.snoop_miss
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_hit_m.snoop_none
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_hit_m.snoop_not_needed
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_hit_m.spl_hit
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_hit_s.any_snoop
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_hit_s.snoop_hit_no_fwd
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_hit_s.snoop_hitm
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_hit_s.snoop_miss
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_hit_s.snoop_none
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_hit_s.snoop_not_needed
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_hit_s.spl_hit
       [Counts demand data reads]
  offcore_response.demand_data_rd.l4_hit_local_l4.any_snoop
       [Counts demand data reads]
  offcore_response.demand_data_rd.l4_hit_local_l4.snoop_hit_no_fwd
       [Counts demand data reads]
  offcore_response.demand_data_rd.l4_hit_local_l4.snoop_hitm
       [Counts demand data reads]
  offcore_response.demand_data_rd.l4_hit_local_l4.snoop_miss
       [Counts demand data reads]
  offcore_response.demand_data_rd.l4_hit_local_l4.snoop_none
       [Counts demand data reads]
  offcore_response.demand_data_rd.l4_hit_local_l4.snoop_not_needed
       [Counts demand data reads]
  offcore_response.demand_data_rd.l4_hit_local_l4.spl_hit
       [Counts demand data reads]
  offcore_response.demand_data_rd.supplier_none.any_snoop
       [Counts demand data reads]
  offcore_response.demand_data_rd.supplier_none.snoop_hit_no_fwd
       [Counts demand data reads]
  offcore_response.demand_data_rd.supplier_none.snoop_hitm
       [Counts demand data reads]
  offcore_response.demand_data_rd.supplier_none.snoop_miss
       [Counts demand data reads]
  offcore_response.demand_data_rd.supplier_none.snoop_none
       [Counts demand data reads]
  offcore_response.demand_data_rd.supplier_none.snoop_not_needed
       [Counts demand data reads]
  offcore_response.demand_data_rd.supplier_none.spl_hit
       [Counts demand data reads]
  offcore_response.demand_rfo.any_response
       [Counts all demand data writes (RFOs) have any response type]
  offcore_response.demand_rfo.l3_hit.any_snoop
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_hit.snoop_hit_no_fwd
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_hit.snoop_hitm
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_hit.snoop_miss
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_hit.snoop_none
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_hit.snoop_not_needed
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_hit.spl_hit
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_hit_e.any_snoop
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_hit_e.snoop_hit_no_fwd
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_hit_e.snoop_hitm
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_hit_e.snoop_miss
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_hit_e.snoop_none
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_hit_e.snoop_not_needed
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_hit_e.spl_hit
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_hit_m.any_snoop
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_hit_m.snoop_hit_no_fwd
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_hit_m.snoop_hitm
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_hit_m.snoop_miss
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_hit_m.snoop_none
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_hit_m.snoop_not_needed
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_hit_m.spl_hit
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_hit_s.any_snoop
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_hit_s.snoop_hit_no_fwd
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_hit_s.snoop_hitm
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_hit_s.snoop_miss
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_hit_s.snoop_none
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_hit_s.snoop_not_needed
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_hit_s.spl_hit
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l4_hit_local_l4.any_snoop
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l4_hit_local_l4.snoop_hit_no_fwd
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l4_hit_local_l4.snoop_hitm
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l4_hit_local_l4.snoop_miss
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l4_hit_local_l4.snoop_none
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l4_hit_local_l4.snoop_not_needed
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l4_hit_local_l4.spl_hit
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.supplier_none.any_snoop
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.supplier_none.snoop_hit_no_fwd
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.supplier_none.snoop_hitm
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.supplier_none.snoop_miss
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.supplier_none.snoop_none
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.supplier_none.snoop_not_needed
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.supplier_none.spl_hit
       [Counts all demand data writes (RFOs)]
  offcore_response.other.any_response
       [Counts any other requests have any response type]
  offcore_response.other.l3_hit.any_snoop
       [Counts any other requests]
  offcore_response.other.l3_hit.snoop_hit_no_fwd
       [Counts any other requests]
  offcore_response.other.l3_hit.snoop_hitm
       [Counts any other requests]
  offcore_response.other.l3_hit.snoop_miss
       [Counts any other requests]
  offcore_response.other.l3_hit.snoop_none
       [Counts any other requests]
  offcore_response.other.l3_hit.snoop_not_needed
       [Counts any other requests]
  offcore_response.other.l3_hit.spl_hit
       [Counts any other requests]
  offcore_response.other.l3_hit_e.any_snoop
       [Counts any other requests]
  offcore_response.other.l3_hit_e.snoop_hit_no_fwd
       [Counts any other requests]
  offcore_response.other.l3_hit_e.snoop_hitm
       [Counts any other requests]
  offcore_response.other.l3_hit_e.snoop_miss
       [Counts any other requests]
  offcore_response.other.l3_hit_e.snoop_none
       [Counts any other requests]
  offcore_response.other.l3_hit_e.snoop_not_needed
       [Counts any other requests]
  offcore_response.other.l3_hit_e.spl_hit
       [Counts any other requests]
  offcore_response.other.l3_hit_m.any_snoop
       [Counts any other requests]
  offcore_response.other.l3_hit_m.snoop_hit_no_fwd
       [Counts any other requests]
  offcore_response.other.l3_hit_m.snoop_hitm
       [Counts any other requests]
  offcore_response.other.l3_hit_m.snoop_miss
       [Counts any other requests]
  offcore_response.other.l3_hit_m.snoop_none
       [Counts any other requests]
  offcore_response.other.l3_hit_m.snoop_not_needed
       [Counts any other requests]
  offcore_response.other.l3_hit_m.spl_hit
       [Counts any other requests]
  offcore_response.other.l3_hit_s.any_snoop
       [Counts any other requests]
  offcore_response.other.l3_hit_s.snoop_hit_no_fwd
       [Counts any other requests]
  offcore_response.other.l3_hit_s.snoop_hitm
       [Counts any other requests]
  offcore_response.other.l3_hit_s.snoop_miss
       [Counts any other requests]
  offcore_response.other.l3_hit_s.snoop_none
       [Counts any other requests]
  offcore_response.other.l3_hit_s.snoop_not_needed
       [Counts any other requests]
  offcore_response.other.l3_hit_s.spl_hit
       [Counts any other requests]
  offcore_response.other.l4_hit_local_l4.any_snoop
       [Counts any other requests]
  offcore_response.other.l4_hit_local_l4.snoop_hit_no_fwd
       [Counts any other requests]
  offcore_response.other.l4_hit_local_l4.snoop_hitm
       [Counts any other requests]
  offcore_response.other.l4_hit_local_l4.snoop_miss
       [Counts any other requests]
  offcore_response.other.l4_hit_local_l4.snoop_none
       [Counts any other requests]
  offcore_response.other.l4_hit_local_l4.snoop_not_needed
       [Counts any other requests]
  offcore_response.other.l4_hit_local_l4.spl_hit
       [Counts any other requests]
  offcore_response.other.supplier_none.any_snoop
       [Counts any other requests]
  offcore_response.other.supplier_none.snoop_hit_no_fwd
       [Counts any other requests]
  offcore_response.other.supplier_none.snoop_hitm
       [Counts any other requests]
  offcore_response.other.supplier_none.snoop_miss
       [Counts any other requests]
  offcore_response.other.supplier_none.snoop_none
       [Counts any other requests]
  offcore_response.other.supplier_none.snoop_not_needed
       [Counts any other requests]
  offcore_response.other.supplier_none.spl_hit
       [Counts any other requests]
  sq_misc.split_lock
       [Number of cache line split locks sent to uncore]
  sw_prefetch_access.nta
       [Number of PREFETCHNTA instructions executed]
  sw_prefetch_access.prefetchw
       [Number of PREFETCHW instructions executed]
  sw_prefetch_access.t0
       [Number of PREFETCHT0 instructions executed]
  sw_prefetch_access.t1_t2
       [Number of PREFETCHT1 or PREFETCHT2 instructions executed]

floating point:
  fp_arith_inst_retired.128b_packed_double
       [Counts once for most SIMD 128-bit packed computational double
        precision floating-point instructions retired. Counts twice for DPP
        and FM(N)ADD/SUB instructions retired]
  fp_arith_inst_retired.128b_packed_single
       [Counts once for most SIMD 128-bit packed computational single
        precision floating-point instruction retired. Counts twice for DPP and
        FM(N)ADD/SUB instructions retired]
  fp_arith_inst_retired.256b_packed_double
       [Counts once for most SIMD 256-bit packed double computational
        precision floating-point instructions retired. Counts twice for DPP
        and FM(N)ADD/SUB instructions retired]
  fp_arith_inst_retired.256b_packed_single
       [Counts once for most SIMD 256-bit packed single computational
        precision floating-point instructions retired. Counts twice for DPP
        and FM(N)ADD/SUB instructions retired]
  fp_arith_inst_retired.4_flops
       [Number of SSE/AVX computational 128-bit packed single and 256-bit
        packed double precision FP instructions retired; some instructions
        will count twice as noted below. Each count represents 2 or/and 4
        computation operations, 1 for each element. Applies to SSE* and AVX*
        packed single precision and packed double precision FP instructions:
        ADD SUB HADD HSUB SUBADD MUL DIV MIN MAX RCP14 RSQRT14 SQRT DPP
        FM(N)ADD/SUB. DPP and FM(N)ADD/SUB count twice as they perform 2
        calculations per element]
  fp_arith_inst_retired.scalar
       [Counts once for most SIMD scalar computational floating-point
        instructions retired. Counts twice for DPP and FM(N)ADD/SUB
        instructions retired]
  fp_arith_inst_retired.scalar_double
       [Counts once for most SIMD scalar computational double precision
        floating-point instructions retired. Counts twice for DPP and
        FM(N)ADD/SUB instructions retired]
  fp_arith_inst_retired.scalar_single
       [Counts once for most SIMD scalar computational single precision
        floating-point instructions retired. Counts twice for DPP and
        FM(N)ADD/SUB instructions retired]
  fp_arith_inst_retired.vector
       [Number of any Vector retired FP arithmetic instructions]
  fp_assist.any
       [Cycles with any input/output SSE or FP assist]

frontend:
  baclears.any
       [Counts the total number when the front end is resteered, mainly when
        the BPU cannot provide a correct prediction and this is corrected by
        other branch handling mechanisms at the front end]
  decode.lcp
       [Stalls caused by changing prefix length of the instruction. [This
        event is alias to ILD_STALL.LCP]]
  dsb2mite_switches.count
       [Decode Stream Buffer (DSB)-to-MITE switches]
  dsb2mite_switches.penalty_cycles
       [Decode Stream Buffer (DSB)-to-MITE switch true penalty cycles]
  frontend_retired.any_dsb_miss
       [Retired Instructions who experienced DSB miss (Precise event)]
  frontend_retired.dsb_miss
       [Retired Instructions who experienced a critical DSB miss (Precise
        event)]
  frontend_retired.itlb_miss
       [Retired Instructions who experienced iTLB true miss (Precise event)]
  frontend_retired.l1i_miss
       [Retired Instructions who experienced Instruction L1 Cache true miss
        (Precise event)]
  frontend_retired.l2_miss
       [Retired Instructions who experienced Instruction L2 Cache true miss
        (Precise event)]
  frontend_retired.latency_ge_1
       [Retired instructions after front-end starvation of at least 1 cycle
        (Must be precise)]
  frontend_retired.latency_ge_128
       [Retired instructions that are fetched after an interval where the
        front-end delivered no uops for a period of 128 cycles which was not
        interrupted by a back-end stall (Precise event)]
  frontend_retired.latency_ge_16
       [Retired instructions that are fetched after an interval where the
        front-end delivered no uops for a period of 16 cycles which was not
        interrupted by a back-end stall (Precise event)]
  frontend_retired.latency_ge_2
       [Retired instructions that are fetched after an interval where the
        front-end delivered no uops for a period of 2 cycles which was not
        interrupted by a back-end stall (Precise event)]
  frontend_retired.latency_ge_256
       [Retired instructions that are fetched after an interval where the
        front-end delivered no uops for a period of 256 cycles which was not
        interrupted by a back-end stall (Precise event)]
  frontend_retired.latency_ge_2_bubbles_ge_1
       [Retired instructions that are fetched after an interval where the
        front-end had at least 1 bubble-slot for a period of 2 cycles which
        was not interrupted by a back-end stall (Precise event)]
  frontend_retired.latency_ge_2_bubbles_ge_2
       [Retired instructions that are fetched after an interval where the
        front-end had at least 2 bubble-slots for a period of 2 cycles which
        was not interrupted by a back-end stall (Precise event)]
  frontend_retired.latency_ge_2_bubbles_ge_3
       [Retired instructions that are fetched after an interval where the
        front-end had at least 3 bubble-slots for a period of 2 cycles which
        was not interrupted by a back-end stall (Precise event)]
  frontend_retired.latency_ge_32
       [Retired instructions that are fetched after an interval where the
        front-end delivered no uops for a period of 32 cycles which was not
        interrupted by a back-end stall (Precise event)]
  frontend_retired.latency_ge_4
       [Retired instructions that are fetched after an interval where the
        front-end delivered no uops for a period of 4 cycles which was not
        interrupted by a back-end stall (Precise event)]
  frontend_retired.latency_ge_512
       [Retired instructions that are fetched after an interval where the
        front-end delivered no uops for a period of 512 cycles which was not
        interrupted by a back-end stall (Precise event)]
  frontend_retired.latency_ge_64
       [Retired instructions that are fetched after an interval where the
        front-end delivered no uops for a period of 64 cycles which was not
        interrupted by a back-end stall (Precise event)]
  frontend_retired.latency_ge_8
       [Retired instructions that are fetched after an interval where the
        front-end delivered no uops for a period of 8 cycles which was not
        interrupted by a back-end stall (Precise event)]
  frontend_retired.stlb_miss
       [Retired Instructions who experienced STLB (2nd level TLB) true miss
        (Precise event)]
  icache_16b.ifdata_stall
       [Cycles where a code fetch is stalled due to L1 instruction cache miss]
  icache_64b.iftag_hit
       [Instruction fetch tag lookups that hit in the instruction cache (L1I).
        Counts at 64-byte cache-line granularity]
  icache_64b.iftag_miss
       [Instruction fetch tag lookups that miss in the instruction cache
        (L1I). Counts at 64-byte cache-line granularity]
  icache_64b.iftag_stall
       [Cycles where a code fetch is stalled due to L1 instruction cache tag
        miss. [This event is alias to ICACHE_TAG.STALLS]]
  icache_tag.stalls
       [Cycles where a code fetch is stalled due to L1 instruction cache tag
        miss. [This event is alias to ICACHE_64B.IFTAG_STALL]]
  idq.all_dsb_cycles_4_uops
       [Cycles Decode Stream Buffer (DSB) is delivering 4 Uops [This event is
        alias to IDQ.DSB_CYCLES_OK]]
  idq.all_dsb_cycles_any_uops
       [Cycles Decode Stream Buffer (DSB) is delivering any Uop [This event is
        alias to IDQ.DSB_CYCLES_ANY]]
  idq.all_mite_cycles_4_uops
       [Cycles MITE is delivering 4 Uops]
  idq.all_mite_cycles_any_uops
       [Cycles MITE is delivering any Uop]
  idq.dsb_cycles
       [Cycles when uops are being delivered to Instruction Decode Queue (IDQ)
        from Decode Stream Buffer (DSB) path]
  idq.dsb_cycles_any
       [Cycles Decode Stream Buffer (DSB) is delivering any Uop [This event is
        alias to IDQ.ALL_DSB_CYCLES_ANY_UOPS]]
  idq.dsb_cycles_ok
       [Cycles Decode Stream Buffer (DSB) is delivering 4 Uops [This event is
        alias to IDQ.ALL_DSB_CYCLES_4_UOPS]]
  idq.dsb_uops
       [Uops delivered to Instruction Decode Queue (IDQ) from the Decode
        Stream Buffer (DSB) path]
  idq.mite_cycles
       [Cycles when uops are being delivered to Instruction Decode Queue (IDQ)
        from MITE path]
  idq.mite_uops
       [Uops delivered to Instruction Decode Queue (IDQ) from MITE path]
  idq.ms_cycles
       [Cycles when uops are being delivered to Instruction Decode Queue (IDQ)
        while Microcode Sequencer (MS) is busy]
  idq.ms_dsb_cycles
       [Cycles when uops initiated by Decode Stream Buffer (DSB) are being
        delivered to Instruction Decode Queue (IDQ) while Microcode Sequencer
        (MS) is busy]
  idq.ms_mite_uops
       [Uops initiated by MITE and delivered to Instruction Decode Queue (IDQ)
        while Microcode Sequencer (MS) is busy]
  idq.ms_switches
       [Number of switches from DSB (Decode Stream Buffer) or MITE (legacy
        decode pipeline) to the Microcode Sequencer]
  idq.ms_uops
       [Uops delivered to Instruction Decode Queue (IDQ) while Microcode
        Sequencer (MS) is busy]
  idq_uops_not_delivered.core
       [Uops not delivered to Resource Allocation Table (RAT) per thread when
        backend of the machine is not stalled]
  idq_uops_not_delivered.cycles_0_uops_deliv.core
       [Cycles per thread when 4 or more uops are not delivered to Resource
        Allocation Table (RAT) when backend of the machine is not stalled]
  idq_uops_not_delivered.cycles_fe_was_ok
       [Counts cycles FE delivered 4 uops or Resource Allocation Table (RAT)
        was stalling FE]
  idq_uops_not_delivered.cycles_le_1_uop_deliv.core
       [Cycles per thread when 3 or more uops are not delivered to Resource
        Allocation Table (RAT) when backend of the machine is not stalled]
  idq_uops_not_delivered.cycles_le_2_uop_deliv.core
       [Cycles with less than 2 uops delivered by the front end]
  idq_uops_not_delivered.cycles_le_3_uop_deliv.core
       [Cycles with less than 3 uops delivered by the front end]

memory:
  cycle_activity.cycles_l3_miss
       [Cycles while L3 cache miss demand load is outstanding]
  cycle_activity.stalls_l3_miss
       [Execution stalls while L3 cache miss demand load is outstanding]
  hle_retired.aborted
       [Number of times an HLE execution aborted due to any reasons (multiple
        categories may count as one) (Precise event)]
  hle_retired.aborted_events
       [Number of times an HLE execution aborted due to unfriendly events
        (such as interrupts)]
  hle_retired.aborted_mem
       [Number of times an HLE execution aborted due to various memory events
        (e.g., read/write capacity and conflicts)]
  hle_retired.aborted_memtype
       [Number of times an HLE execution aborted due to incompatible memory
        type]
  hle_retired.aborted_timer
       [Number of times an HLE execution aborted due to hardware timer
        expiration]
  hle_retired.aborted_unfriendly
       [Number of times an HLE execution aborted due to HLE-unfriendly
        instructions and certain unfriendly events (such as AD assists etc.)]
  hle_retired.commit
       [Number of times an HLE execution successfully committed]
  hle_retired.start
       [Number of times an HLE execution started]
  machine_clears.memory_ordering
       [Counts the number of machine clears due to memory order conflicts Spec
        update: SKL089]
  mem_trans_retired.load_latency_gt_128
       [Counts randomly selected loads when the latency from first dispatch to
        completion is greater than 128 cycles Supports address when precise
        (Must be precise)]
  mem_trans_retired.load_latency_gt_16
       [Counts randomly selected loads when the latency from first dispatch to
        completion is greater than 16 cycles Supports address when precise
        (Must be precise)]
  mem_trans_retired.load_latency_gt_256
       [Counts randomly selected loads when the latency from first dispatch to
        completion is greater than 256 cycles Supports address when precise
        (Must be precise)]
  mem_trans_retired.load_latency_gt_32
       [Counts randomly selected loads when the latency from first dispatch to
        completion is greater than 32 cycles Supports address when precise
        (Must be precise)]
  mem_trans_retired.load_latency_gt_4
       [Counts randomly selected loads when the latency from first dispatch to
        completion is greater than 4 cycles Supports address when precise
        (Must be precise)]
  mem_trans_retired.load_latency_gt_512
       [Counts randomly selected loads when the latency from first dispatch to
        completion is greater than 512 cycles Supports address when precise
        (Must be precise)]
  mem_trans_retired.load_latency_gt_64
       [Counts randomly selected loads when the latency from first dispatch to
        completion is greater than 64 cycles Supports address when precise
        (Must be precise)]
  mem_trans_retired.load_latency_gt_8
       [Counts randomly selected loads when the latency from first dispatch to
        completion is greater than 8 cycles Supports address when precise
        (Must be precise)]
  offcore_requests.l3_miss_demand_data_rd
       [Demand Data Read requests who miss L3 cache]
  offcore_requests_outstanding.cycles_with_l3_miss_demand_data_rd
       [Cycles with at least 1 Demand Data Read requests who miss L3 cache in
        the superQ]
  offcore_requests_outstanding.l3_miss_demand_data_rd
       [Counts number of Offcore outstanding Demand Data Read requests that
        miss L3 cache in the superQ every cycle]
  offcore_requests_outstanding.l3_miss_demand_data_rd_ge_6
       [Cycles with at least 6 Demand Data Read requests that miss L3 cache in
        the superQ]
  offcore_response.demand_code_rd.l3_hit.snoop_non_dram
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_hit_e.snoop_non_dram
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_hit_m.snoop_non_dram
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_hit_s.snoop_non_dram
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_miss.any_snoop
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_miss.snoop_hit_no_fwd
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_miss.snoop_hitm
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_miss.snoop_miss
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_miss.snoop_non_dram
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_miss.snoop_none
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_miss.snoop_not_needed
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_miss.spl_hit
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_miss_local_dram.any_snoop
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_miss_local_dram.snoop_hit_no_fwd
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_miss_local_dram.snoop_hitm
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_miss_local_dram.snoop_miss
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_miss_local_dram.snoop_non_dram
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_miss_local_dram.snoop_none
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_miss_local_dram.snoop_not_needed
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l3_miss_local_dram.spl_hit
       [Counts all demand code reads]
  offcore_response.demand_code_rd.l4_hit_local_l4.snoop_non_dram
       [Counts all demand code reads]
  offcore_response.demand_code_rd.supplier_none.snoop_non_dram
       [Counts all demand code reads]
  offcore_response.demand_data_rd.l3_hit.snoop_non_dram
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_hit_e.snoop_non_dram
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_hit_m.snoop_non_dram
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_hit_s.snoop_non_dram
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_miss.any_snoop
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_miss.snoop_hit_no_fwd
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_miss.snoop_hitm
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_miss.snoop_miss
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_miss.snoop_non_dram
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_miss.snoop_none
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_miss.snoop_not_needed
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_miss.spl_hit
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_miss_local_dram.any_snoop
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_miss_local_dram.snoop_hit_no_fwd
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_miss_local_dram.snoop_hitm
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_miss_local_dram.snoop_miss
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_miss_local_dram.snoop_non_dram
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_miss_local_dram.snoop_none
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_miss_local_dram.snoop_not_needed
       [Counts demand data reads]
  offcore_response.demand_data_rd.l3_miss_local_dram.spl_hit
       [Counts demand data reads]
  offcore_response.demand_data_rd.l4_hit_local_l4.snoop_non_dram
       [Counts demand data reads]
  offcore_response.demand_data_rd.supplier_none.snoop_non_dram
       [Counts demand data reads]
  offcore_response.demand_rfo.l3_hit.snoop_non_dram
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_hit_e.snoop_non_dram
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_hit_m.snoop_non_dram
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_hit_s.snoop_non_dram
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_miss.any_snoop
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_miss.snoop_hit_no_fwd
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_miss.snoop_hitm
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_miss.snoop_miss
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_miss.snoop_non_dram
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_miss.snoop_none
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_miss.snoop_not_needed
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_miss.spl_hit
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_miss_local_dram.any_snoop
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_miss_local_dram.snoop_hit_no_fwd
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_miss_local_dram.snoop_hitm
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_miss_local_dram.snoop_miss
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_miss_local_dram.snoop_non_dram
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_miss_local_dram.snoop_none
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_miss_local_dram.snoop_not_needed
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l3_miss_local_dram.spl_hit
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.l4_hit_local_l4.snoop_non_dram
       [Counts all demand data writes (RFOs)]
  offcore_response.demand_rfo.supplier_none.snoop_non_dram
       [Counts all demand data writes (RFOs)]
  offcore_response.other.l3_hit.snoop_non_dram
       [Counts any other requests]
  offcore_response.other.l3_hit_e.snoop_non_dram
       [Counts any other requests]
  offcore_response.other.l3_hit_m.snoop_non_dram
       [Counts any other requests]
  offcore_response.other.l3_hit_s.snoop_non_dram
       [Counts any other requests]
  offcore_response.other.l3_miss.any_snoop
       [Counts any other requests]
  offcore_response.other.l3_miss.snoop_hit_no_fwd
       [Counts any other requests]
  offcore_response.other.l3_miss.snoop_hitm
       [Counts any other requests]
  offcore_response.other.l3_miss.snoop_miss
       [Counts any other requests]
  offcore_response.other.l3_miss.snoop_non_dram
       [Counts any other requests]
  offcore_response.other.l3_miss.snoop_none
       [Counts any other requests]
  offcore_response.other.l3_miss.snoop_not_needed
       [Counts any other requests]
  offcore_response.other.l3_miss.spl_hit
       [Counts any other requests]
  offcore_response.other.l3_miss_local_dram.any_snoop
       [Counts any other requests]
  offcore_response.other.l3_miss_local_dram.snoop_hit_no_fwd
       [Counts any other requests]
  offcore_response.other.l3_miss_local_dram.snoop_hitm
       [Counts any other requests]
  offcore_response.other.l3_miss_local_dram.snoop_miss
       [Counts any other requests]
  offcore_response.other.l3_miss_local_dram.snoop_non_dram
       [Counts any other requests]
  offcore_response.other.l3_miss_local_dram.snoop_none
       [Counts any other requests]
  offcore_response.other.l3_miss_local_dram.snoop_not_needed
       [Counts any other requests]
  offcore_response.other.l3_miss_local_dram.spl_hit
       [Counts any other requests]
  offcore_response.other.l4_hit_local_l4.snoop_non_dram
       [Counts any other requests]
  offcore_response.other.supplier_none.snoop_non_dram
       [Counts any other requests]
  rtm_retired.aborted
       [Number of times an RTM execution aborted due to any reasons (multiple
        categories may count as one) (Precise event)]
  rtm_retired.aborted_events
       [Number of times an RTM execution aborted due to none of the previous 4
        categories (e.g. interrupt)]
  rtm_retired.aborted_mem
       [Number of times an RTM execution aborted due to various memory events
        (e.g. read/write capacity and conflicts)]
  rtm_retired.aborted_memtype
       [Number of times an RTM execution aborted due to incompatible memory
        type]
  rtm_retired.aborted_timer
       [Number of times an RTM execution aborted due to uncommon conditions]
  rtm_retired.aborted_unfriendly
       [Number of times an RTM execution aborted due to HLE-unfriendly
        instructions]
  rtm_retired.commit
       [Number of times an RTM execution successfully committed]
  rtm_retired.start
       [Number of times an RTM execution started]
  tx_exec.misc1
       [Counts the number of times a class of instructions that may cause a
        transactional abort was executed. Since this is the count of
        execution, it may not always cause a transactional abort]
  tx_exec.misc2
       [Counts the number of times a class of instructions (e.g., vzeroupper)
        that may cause a transactional abort was executed inside a
        transactional region]
  tx_exec.misc3
       [Counts the number of times an instruction execution caused the
        transactional nest count supported to be exceeded]
  tx_exec.misc4
       [Counts the number of times a XBEGIN instruction was executed inside an
        HLE transactional region]
  tx_exec.misc5
       [Counts the number of times an HLE XACQUIRE instruction was executed
        inside an RTM transactional region]
  tx_mem.abort_capacity
       [Number of times a transactional abort was signaled due to a data
        capacity limitation for transactional reads or writes]
  tx_mem.abort_conflict
       [Number of times a transactional abort was signaled due to a data
        conflict on a transactionally accessed address]
  tx_mem.abort_hle_elision_buffer_mismatch
       [Number of times an HLE transactional execution aborted due to XRELEASE
        lock not satisfying the address and value requirements in the elision
        buffer]
  tx_mem.abort_hle_elision_buffer_not_empty
       [Number of times an HLE transactional execution aborted due to
        NoAllocatedElisionBuffer being non-zero]
  tx_mem.abort_hle_elision_buffer_unsupported_alignment
       [Number of times an HLE transactional execution aborted due to an
        unsupported read alignment from the elision buffer]
  tx_mem.abort_hle_store_to_elided_lock
       [Number of times a HLE transactional region aborted due to a non
        XRELEASE prefixed instruction writing to an elided lock in the elision
        buffer]
  tx_mem.hle_elision_buffer_full
       [Number of times HLE lock could not be elided due to
        ElisionBufferAvailable being zero]

other:
  hw_interrupts.received
       [Number of hardware interrupts received by the processor]
  memory_disambiguation.history_reset
       [MEMORY_DISAMBIGUATION.HISTORY_RESET]

pipeline:
  arith.divider_active
       [Cycles when divide unit is busy executing divide or square root
        operations. Accounts for integer and floating-point operations]
  br_inst_retired.all_branches
       [All (macro) branch instructions retired Spec update: SKL091]
  br_inst_retired.all_branches_pebs
       [All (macro) branch instructions retired Spec update: SKL091 (Must be
        precise)]
  br_inst_retired.cond
       [Conditional branch instructions retired. [This event is alias to
        BR_INST_RETIRED.CONDITIONAL] Spec update: SKL091]
  br_inst_retired.cond_ntaken
       [Not taken branch instructions retired Spec update: SKL091]
  br_inst_retired.conditional
       [Conditional branch instructions retired. [This event is alias to
        BR_INST_RETIRED.COND] Spec update: SKL091 (Precise event)]
  br_inst_retired.far_branch
       [Far branch instructions retired Spec update: SKL091 (Precise event)]
  br_inst_retired.near_call
       [Direct and indirect near call instructions retired Spec update: SKL091
        (Precise event)]
  br_inst_retired.near_return
       [Return instructions retired Spec update: SKL091 (Precise event)]
  br_inst_retired.near_taken
       [Taken branch instructions retired Spec update: SKL091 (Precise event)]
  br_inst_retired.not_taken
       [Not taken branch instructions retired Spec update: SKL091]
  br_misp_exec.all_branches
       [Speculative and retired mispredicted macro conditional branches]
  br_misp_exec.indirect
       [Speculative mispredicted indirect branches]
  br_misp_retired.all_branches
       [All mispredicted macro branch instructions retired]
  br_misp_retired.all_branches_pebs
       [Mispredicted macro branch instructions retired (Must be precise)]
  br_misp_retired.conditional
       [Mispredicted conditional branch instructions retired (Precise event)]
  br_misp_retired.near_call
       [Mispredicted direct and indirect near call instructions retired
        (Precise event)]
  br_misp_retired.near_taken
       [Number of near branch instructions retired that were mispredicted and
        taken (Precise event)]
  cpu_clk_thread_unhalted.one_thread_active
       [Core crystal clock cycles when this thread is unhalted and the other
        thread is halted]
  cpu_clk_thread_unhalted.ref_xclk
       [Core crystal clock cycles when the thread is unhalted]
  cpu_clk_thread_unhalted.ref_xclk_any
       [Core crystal clock cycles when at least one thread on the physical
        core is unhalted]
  cpu_clk_unhalted.one_thread_active
       [Core crystal clock cycles when this thread is unhalted and the other
        thread is halted]
  cpu_clk_unhalted.ref_tsc
       [Reference cycles when the core is not in halt state]
  cpu_clk_unhalted.ref_xclk
       [Core crystal clock cycles when the thread is unhalted]
  cpu_clk_unhalted.ref_xclk_any
       [Core crystal clock cycles when at least one thread on the physical
        core is unhalted]
  cpu_clk_unhalted.ring0_trans
       [Counts when there is a transition from ring 1, 2 or 3 to ring 0]
  cpu_clk_unhalted.thread
       [Core cycles when the thread is not in halt state]
  cpu_clk_unhalted.thread_any
       [Core cycles when at least one thread on the physical core is not in
        halt state]
  cpu_clk_unhalted.thread_p
       [Thread cycles when thread is not in halt state]
  cpu_clk_unhalted.thread_p_any
       [Core cycles when at least one thread on the physical core is not in
        halt state]
  cycle_activity.cycles_l1d_miss
       [Cycles while L1 cache miss demand load is outstanding]
  cycle_activity.cycles_l2_miss
       [Cycles while L2 cache miss demand load is outstanding]
  cycle_activity.cycles_mem_any
       [Cycles while memory subsystem has an outstanding load]
  cycle_activity.stalls_l1d_miss
       [Execution stalls while L1 cache miss demand load is outstanding]
  cycle_activity.stalls_l2_miss
       [Execution stalls while L2 cache miss demand load is outstanding]
  cycle_activity.stalls_mem_any
       [Execution stalls while memory subsystem has an outstanding load]
  cycle_activity.stalls_total
       [Total execution stalls]
  exe_activity.1_ports_util
       [Cycles total of 1 uop is executed on all ports and Reservation Station
        was not empty]
  exe_activity.2_ports_util
       [Cycles total of 2 uops are executed on all ports and Reservation
        Station was not empty]
  exe_activity.3_ports_util
       [Cycles total of 3 uops are executed on all ports and Reservation
        Station was not empty]
  exe_activity.4_ports_util
       [Cycles total of 4 uops are executed on all ports and Reservation
        Station was not empty]
  exe_activity.bound_on_stores
       [Cycles where the Store Buffer was full and no outstanding load]
  exe_activity.exe_bound_0_ports
       [Cycles where no uops were executed, the Reservation Station was not
        empty, the Store Buffer was full and there was no outstanding load]
  ild_stall.lcp
       [Stalls caused by changing prefix length of the instruction. [This
        event is alias to DECODE.LCP]]
  inst_decoded.decoders
       [Instruction decoders utilized in a cycle]
  inst_retired.any
       [Instructions retired from execution]
  inst_retired.any_p
       [Number of instructions retired. General Counter - architectural event
        Spec update: SKL091, SKL044]
  inst_retired.nop
       [Number of all retired NOP instructions Spec update: SKL091, SKL044
        (Precise event)]
  inst_retired.prec_dist
       [Precise instruction retired event with HW to reduce effect of PEBS
        shadow in IP distribution Spec update: SKL091, SKL044 (Must be
        precise)]
  inst_retired.total_cycles_ps
       [Number of cycles using always true condition applied to PEBS
        instructions retired event Spec update: SKL091, SKL044 (Must be
        precise)]
  int_misc.clear_resteer_cycles
       [Cycles the issue-stage is waiting for front-end to fetch from
        resteered path following branch misprediction or machine clear events]
  int_misc.clears_count
       [Clears speculative count]
  int_misc.recovery_cycles
       [Core cycles the allocator was stalled due to recovery from earlier
        clear event for this thread (e.g. misprediction or memory nuke)]
  int_misc.recovery_cycles_any
       [Core cycles the allocator was stalled due to recovery from earlier
        clear event for any thread running on the physical core (e.g.
        misprediction or memory nuke)]
  ld_blocks.no_sr
       [The number of times that split load operations are temporarily blocked
        because all resources for handling the split accesses are in use]
  ld_blocks.store_forward
       [Loads blocked due to overlapping with a preceding store that cannot be
        forwarded]
  ld_blocks_partial.address_alias
       [False dependencies in MOB due to partial compare on address]
  load_hit_pre.sw_pf
       [Demand load dispatches that hit L1D fill buffer (FB) allocated for
        software prefetch]
  lsd.cycles_4_uops
       [Cycles 4 Uops delivered by the LSD, but didn't come from the decoder.
        [This event is alias to LSD.CYCLES_OK]]
  lsd.cycles_active
       [Cycles Uops delivered by the LSD, but didn't come from the decoder]
  lsd.cycles_ok
       [Cycles 4 Uops delivered by the LSD, but didn't come from the decoder.
        [This event is alias to LSD.CYCLES_4_UOPS]]
  lsd.uops
       [Number of Uops delivered by the LSD]
  machine_clears.count
       [Number of machine clears (nukes) of any type]
  machine_clears.smc
       [Self-modifying code (SMC) detected]
  other_assists.any
       [Number of times a microcode assist is invoked by HW other than
        FP-assist. Examples include AD (page Access Dirty) and AVX* related
        assists]
  partial_rat_stalls.scoreboard
       [Cycles where the pipeline is stalled due to serializing operations]
  resource_stalls.any
       [Resource-related stall cycles]
  resource_stalls.sb
       [Cycles stalled due to no store buffers available. (not including
        draining form sync)]
  rob_misc_events.lbr_inserts
       [Increments whenever there is an update to the LBR array]
  rob_misc_events.pause_inst
       [Number of retired PAUSE instructions (that do not end up with a VMExit
        to the VMM; TSX aborted Instructions may be counted). This event is
        not supported on first SKL and KBL products]
  rs_events.empty_cycles
       [Cycles when Reservation Station (RS) is empty for the thread]
  rs_events.empty_end
       [Counts end of periods where the Reservation Station (RS) was empty.
        Could be useful to precisely locate Frontend Latency Bound issues]
  uops_dispatched_port.port_0
       [Cycles per thread when uops are executed in port 0]
  uops_dispatched_port.port_1
       [Cycles per thread when uops are executed in port 1]
  uops_dispatched_port.port_2
       [Cycles per thread when uops are executed in port 2]
  uops_dispatched_port.port_3
       [Cycles per thread when uops are executed in port 3]
  uops_dispatched_port.port_4
       [Cycles per thread when uops are executed in port 4]
  uops_dispatched_port.port_5
       [Cycles per thread when uops are executed in port 5]
  uops_dispatched_port.port_6
       [Cycles per thread when uops are executed in port 6]
  uops_dispatched_port.port_7
       [Cycles per thread when uops are executed in port 7]
  uops_executed.core
       [Number of uops executed on the core]
  uops_executed.core_cycles_ge_1
       [Cycles at least 1 micro-op is executed from any thread on physical
        core]
  uops_executed.core_cycles_ge_2
       [Cycles at least 2 micro-op is executed from any thread on physical
        core]
  uops_executed.core_cycles_ge_3
       [Cycles at least 3 micro-op is executed from any thread on physical
        core]
  uops_executed.core_cycles_ge_4
       [Cycles at least 4 micro-op is executed from any thread on physical
        core]
  uops_executed.core_cycles_none
       [Cycles with no micro-ops executed from any thread on physical core]
  uops_executed.cycles_ge_1_uop_exec
       [Cycles where at least 1 uop was executed per-thread]
  uops_executed.cycles_ge_2_uops_exec
       [Cycles where at least 2 uops were executed per-thread]
  uops_executed.cycles_ge_3_uops_exec
       [Cycles where at least 3 uops were executed per-thread]
  uops_executed.cycles_ge_4_uops_exec
       [Cycles where at least 4 uops were executed per-thread]
  uops_executed.stall_cycles
       [Counts number of cycles no uops were dispatched to be executed on this
        thread]
  uops_executed.thread
       [Counts the number of uops to be executed per-thread each cycle]
  uops_executed.x87
       [Counts the number of x87 uops dispatched]
  uops_issued.any
       [Uops that Resource Allocation Table (RAT) issues to Reservation
        Station (RS)]
  uops_issued.slow_lea
       [Number of slow LEA uops being allocated. A uop is generally considered
        SlowLea if it has 3 sources (e.g. 2 sources + immediate) regardless if
        as a result of LEA instruction or not]
  uops_issued.stall_cycles
       [Cycles when Resource Allocation Table (RAT) does not issue Uops to
        Reservation Station (RS) for the thread]
  uops_issued.vector_width_mismatch
       [Uops inserted at issue-stage in order to preserve upper bits of vector
        registers]
  uops_retired.macro_fused
       [Number of macro-fused uops retired. (non precise)]
  uops_retired.retire_slots
       [Retirement slots used]
  uops_retired.stall_cycles
       [Cycles without actually retired uops]
  uops_retired.total_cycles
       [Cycles with less than 10 actually retired uops]

uncore cache:
  unc_cbo_cache_lookup.any_es
       [L3 Lookup any request that access cache and found line in E or
        S-state. Unit: uncore_cbox]
  unc_cbo_cache_lookup.any_i
       [L3 Lookup any request that access cache and found line in I-state.
        Unit: uncore_cbox]
  unc_cbo_cache_lookup.any_m
       [L3 Lookup any request that access cache and found line in M-state.
        Unit: uncore_cbox]
  unc_cbo_cache_lookup.any_mesi
       [L3 Lookup any request that access cache and found line in MESI-state.
        Unit: uncore_cbox]
  unc_cbo_cache_lookup.read_es
       [L3 Lookup read request that access cache and found line in E or
        S-state. Unit: uncore_cbox]
  unc_cbo_cache_lookup.read_i
       [L3 Lookup read request that access cache and found line in I-state.
        Unit: uncore_cbox]
  unc_cbo_cache_lookup.read_mesi
       [L3 Lookup read request that access cache and found line in any
        MESI-state. Unit: uncore_cbox]
  unc_cbo_cache_lookup.write_es
       [L3 Lookup write request that access cache and found line in E or
        S-state. Unit: uncore_cbox]
  unc_cbo_cache_lookup.write_m
       [L3 Lookup write request that access cache and found line in M-state.
        Unit: uncore_cbox]
  unc_cbo_cache_lookup.write_mesi
       [L3 Lookup write request that access cache and found line in
        MESI-state. Unit: uncore_cbox]
  unc_cbo_xsnp_response.hit_xcore
       [A cross-core snoop initiated by this Cbox due to processor core memory
        request which hits a non-modified line in some processor core. Unit:
        uncore_cbox]
  unc_cbo_xsnp_response.hitm_xcore
       [A cross-core snoop initiated by this Cbox due to processor core memory
        request which hits a modified line in some processor core. Unit:
        uncore_cbox]
  unc_cbo_xsnp_response.miss_eviction
       [A cross-core snoop resulted from L3 Eviction which misses in some
        processor core. Unit: uncore_cbox]
  unc_cbo_xsnp_response.miss_xcore
       [A cross-core snoop initiated by this Cbox due to processor core memory
        request which misses in some processor core. Unit: uncore_cbox]

uncore interconnect:
  unc_arb_coh_trk_requests.all
       [Number of entries allocated. Account for Any type: e.g. Snoop, Core
        aperture, etc. Unit: uncore_arb]
  unc_arb_trk_occupancy.all
       [Number of all Core entries outstanding for the memory controller. The
        outstanding interval starts after LLC miss till return of first data
        chunk. Accounts for Coherent and non-coherent traffic. Unit:
        uncore_arb]
  unc_arb_trk_occupancy.cycles_with_any_request
       [Cycles with at least one request outstanding is waiting for data
        return from memory controller. Account for coherent and non-coherent
        requests initiated by IA Cores, Processor Graphics Unit, or LLC. Unit:
        uncore_arb]
  unc_arb_trk_occupancy.data_read
       [Number of Core Data Read entries outstanding for the memory
        controller. The outstanding interval starts after LLC miss till return
        of first data chunk. Unit: uncore_arb]
  unc_arb_trk_requests.all
       [UNC_ARB_TRK_REQUESTS.ALL. Unit: uncore_arb]
  unc_arb_trk_requests.data_read
       [Number of Core coherent Data Read requests sent to memory controller
        whose data is returned directly to requesting agent. Unit: uncore_arb]
  unc_arb_trk_requests.drd_direct
       [Number of Core coherent Data Read requests sent to memory controller
        whose data is returned directly to requesting agent. Unit: uncore_arb]
  unc_arb_trk_requests.writes
       [Number of Writes allocated - any write transactions: full/partials
        writes and evictions. Unit: uncore_arb]

virtual memory:
  dtlb_load_misses.miss_causes_a_walk
       [Load misses in all DTLB levels that cause page walks]
  dtlb_load_misses.stlb_hit
       [Loads that miss the DTLB and hit the STLB]
  dtlb_load_misses.walk_active
       [Cycles when at least one PMH is busy with a page walk for a load. EPT
        page walk duration are excluded in Skylake]
  dtlb_load_misses.walk_completed
       [Load miss in all TLB levels causes a page walk that completes. (All
        page sizes)]
  dtlb_load_misses.walk_completed_1g
       [Page walk completed due to a demand data load to a 1G page]
  dtlb_load_misses.walk_completed_2m_4m
       [Page walk completed due to a demand data load to a 2M/4M page]
  dtlb_load_misses.walk_completed_4k
       [Page walk completed due to a demand data load to a 4K page]
  dtlb_load_misses.walk_pending
       [Counts 1 per cycle for each PMH that is busy with a page walk for a
        load. EPT page walk duration are excluded in Skylake]
  dtlb_store_misses.miss_causes_a_walk
       [Store misses in all DTLB levels that cause page walks]
  dtlb_store_misses.stlb_hit
       [Stores that miss the DTLB and hit the STLB]
  dtlb_store_misses.walk_active
       [Cycles when at least one PMH is busy with a page walk for a store. EPT
        page walk duration are excluded in Skylake]
  dtlb_store_misses.walk_completed
       [Store misses in all TLB levels causes a page walk that completes. (All
        page sizes)]
  dtlb_store_misses.walk_completed_1g
       [Page walk completed due to a demand data store to a 1G page]
  dtlb_store_misses.walk_completed_2m_4m
       [Page walk completed due to a demand data store to a 2M/4M page]
  dtlb_store_misses.walk_completed_4k
       [Page walk completed due to a demand data store to a 4K page]
  dtlb_store_misses.walk_pending
       [Counts 1 per cycle for each PMH that is busy with a page walk for a
        store. EPT page walk duration are excluded in Skylake]
  ept.walk_pending
       [Counts 1 per cycle for each PMH that is busy with a EPT (Extended Page
        Table) walk for any request type]
  itlb.itlb_flush
       [Flushing of the Instruction TLB (ITLB) pages, includes 4k/2M/4M pages]
  itlb_misses.miss_causes_a_walk
       [Misses at all ITLB levels that cause page walks]
  itlb_misses.stlb_hit
       [Instruction fetch requests that miss the ITLB and hit the STLB]
  itlb_misses.walk_active
       [Cycles when at least one PMH is busy with a page walk for code
        (instruction fetch) request. EPT page walk duration are excluded in
        Skylake]
  itlb_misses.walk_completed
       [Code miss in all TLB levels causes a page walk that completes. (All
        page sizes)]
  itlb_misses.walk_completed_1g
       [Code miss in all TLB levels causes a page walk that completes. (1G)]
  itlb_misses.walk_completed_2m_4m
       [Code miss in all TLB levels causes a page walk that completes. (2M/4M)]
  itlb_misses.walk_completed_4k
       [Code miss in all TLB levels causes a page walk that completes. (4K)]
  itlb_misses.walk_pending
       [Counts 1 per cycle for each PMH that is busy with a page walk for an
        instruction fetch request. EPT page walk duration are excluded in
        Skylake]
  tlb_flush.dtlb_thread
       [DTLB flush attempts of the thread-specific entries]
  tlb_flush.stlb_any
       [STLB flush attempts]
  rNNN                                               [Raw hardware event descriptor]
  cpu/t1=v1[,t2=v2,t3 ...]/modifier                  [Raw hardware event descriptor]
       [(see 'man perf-list' on how to encode it)]
  mem:<addr>[/len][:access]                          [Hardware breakpoint]
Error: failed to open tracing events directory

Metric Groups:

Backend: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_core_bound
       [This metric represents fraction of slots where Core non-memory issues
        were of a bottleneck]
  tma_info_core_ilp
       [Instruction-Level-Parallelism (average number of uops executed when
        there is execution) per-core]
  tma_info_memory_l2mpki
       [L2 cache true misses per kilo instruction for retired demand loads]
  tma_memory_bound
       [This metric represents fraction of slots the Memory subsystem within
        the Backend was a bottleneck]

Bad: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_bad_spec_branch_misprediction_cost
       [Branch Misprediction Cost: Fraction of TMA slots wasted per
        non-speculative branch misprediction (retired JEClear)]
  tma_info_bad_spec_ipmisp_indirect
       [Instructions per retired mispredicts for indirect CALL or JMP branches
        (lower number means higher occurrence rate)]
  tma_info_bad_spec_ipmispredict
       [Number of Instructions per non-speculative Branch Misprediction
        (JEClear) (lower number means higher occurrence rate)]
  tma_info_bottleneck_mispredictions
       [Total pipeline cost of Branch Misprediction related bottlenecks]
  tma_info_branches_callret
       [Fraction of branches that are CALL or RET]
  tma_info_branches_cond_nt
       [Fraction of branches that are non-taken conditionals]
  tma_info_branches_cond_tk
       [Fraction of branches that are taken conditionals]
  tma_info_branches_jump
       [Fraction of branches that are unconditional (direct or indirect) jumps]

BadSpec:
  tma_branch_mispredicts
       [This metric represents fraction of slots the CPU has wasted due to
        Branch Misprediction]
  tma_clears_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Machine Clears]
  tma_info_bad_spec_ipmispredict
       [Number of Instructions per non-speculative Branch Misprediction
        (JEClear) (lower number means higher occurrence rate)]
  tma_info_bottleneck_mispredictions
       [Total pipeline cost of Branch Misprediction related bottlenecks]
  tma_machine_clears
       [This metric represents fraction of slots the CPU has wasted due to
        Machine Clears]
  tma_mispredicts_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Branch Misprediction at execution stage]

BigFoot: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_icache_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        instruction cache misses]
  tma_info_bottleneck_big_code
       [Total pipeline cost of instruction fetch related bottlenecks by large
        code footprint programs (i-side cache; TLB and BTB misses)]
  tma_itlb_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        Instruction TLB (ITLB) misses]
  tma_unknown_branches
       [This metric represents fraction of cycles the CPU was stalled due to
        new branch address clears]

BrMispredicts: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_branch_mispredicts
       [This metric represents fraction of slots the CPU has wasted due to
        Branch Misprediction]
  tma_info_bad_spec_branch_misprediction_cost
       [Branch Misprediction Cost: Fraction of TMA slots wasted per
        non-speculative branch misprediction (retired JEClear)]
  tma_info_bad_spec_ipmisp_indirect
       [Instructions per retired mispredicts for indirect CALL or JMP branches
        (lower number means higher occurrence rate)]
  tma_info_bad_spec_ipmispredict
       [Number of Instructions per non-speculative Branch Misprediction
        (JEClear) (lower number means higher occurrence rate)]
  tma_info_bottleneck_mispredictions
       [Total pipeline cost of Branch Misprediction related bottlenecks]
  tma_mispredicts_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Branch Misprediction at execution stage]

Branches: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_branches_callret
       [Fraction of branches that are CALL or RET]
  tma_info_branches_cond_nt
       [Fraction of branches that are non-taken conditionals]
  tma_info_branches_cond_tk
       [Fraction of branches that are taken conditionals]
  tma_info_branches_jump
       [Fraction of branches that are unconditional (direct or indirect) jumps]
  tma_info_inst_mix_bptkbranch
       [Branch instructions per taken branch]
  tma_info_inst_mix_ipbranch
       [Instructions per Branch (lower number means higher occurrence rate)]
  tma_info_inst_mix_ipcall
       [Instructions per (near) call (lower number means higher occurrence
        rate)]
  tma_info_inst_mix_iptb
       [Instruction per taken branch]
  tma_info_system_ipfarbranch
       [Instructions per Far Branch ( Far Branches apply upon transition from
        application to operating system, handling interrupts, exceptions)
        [lower number means higher occurrence rate]]
  tma_info_thread_uptb
       [Instruction per taken branch]

CacheMisses: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_memory_fb_hpki
       [Fill Buffer (FB) hits per kilo instructions for retired demand loads
        (L1D misses that merge into ongoing miss-handling entries)]
  tma_info_memory_l1mpki
       [L1 cache true misses per kilo instruction for retired demand loads]
  tma_info_memory_l1mpki_load
       [L1 cache true misses per kilo instruction for all demand loads
        (including speculative)]
  tma_info_memory_l2hpki_all
       [L2 cache hits per kilo instruction for all request types (including
        speculative)]
  tma_info_memory_l2hpki_load
       [L2 cache hits per kilo instruction for all demand loads (including
        speculative)]
  tma_info_memory_l2mpki
       [L2 cache true misses per kilo instruction for retired demand loads]
  tma_info_memory_l2mpki_all
       [L2 cache ([RKL+] true) misses per kilo instruction for all request
        types (including speculative)]
  tma_info_memory_l2mpki_load
       [L2 cache ([RKL+] true) misses per kilo instruction for all demand
        loads (including speculative)]
  tma_info_memory_l3mpki
       [L3 cache true misses per kilo instruction for retired demand loads]
  tma_l1_bound
       [This metric estimates how often the CPU was stalled without loads
        missing the L1 data cache]
  tma_l2_bound
       [This metric estimates how often the CPU was stalled due to L2 cache
        accesses by loads]
  tma_l3_bound
       [This metric estimates how often the CPU was stalled due to loads
        accesses to L3 cache or contended with a sibling Core]

CodeGen: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_branches_cond_nt
       [Fraction of branches that are non-taken conditionals]
  tma_info_branches_cond_tk
       [Fraction of branches that are taken conditionals]

Compute: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_core_bound
       [This metric represents fraction of slots where Core non-memory issues
        were of a bottleneck]
  tma_fp_scalar
       [This metric approximates arithmetic floating-point (FP) scalar uops
        fraction the CPU has retired]
  tma_fp_vector
       [This metric approximates arithmetic floating-point (FP) vector uops
        fraction the CPU has retired aggregated across all vector widths]
  tma_fp_vector_128b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 128-bit wide vectors]
  tma_fp_vector_256b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 256-bit wide vectors]
  tma_port_0
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 0 ([SNB+] ALU; [HSW+] ALU and 2nd branch)]
  tma_x87_use
       [This metric serves as an approximation of legacy x87 usage]

Cor: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_botlnk_l0_core_bound_likely
       [Probability of Core Bound bottleneck hidden by SMT-profiling artifacts]
  tma_info_core_fp_arith_utilization
       [Actual per-core usage of the Floating Point non-X87 execution units
        (regardless of precision or vector-width)]
  tma_info_core_ilp
       [Instruction-Level-Parallelism (average number of uops executed when
        there is execution) per-core]
  tma_info_pipeline_execute
       [Instruction-Level-Parallelism (average number of uops executed when
        there is execution) per-thread]
  tma_info_system_gflops
       [Giga Floating Point Operations Per Second]
  tma_info_thread_execute_per_issue
       [The ratio of Executed- by Issued-Uops]

DSB: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_dsb
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to DSB (decoded uop cache) fetch pipeline]
  tma_info_frontend_dsb_coverage
       [Fraction of Uops delivered by the DSB (aka Decoded ICache; or Uop
        Cache)]

DSBmiss: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_decoder0_alone
       [This metric represents fraction of cycles where decoder-0 was the only
        active decoder]
  tma_dsb_switches
       [This metric represents fraction of cycles the CPU was stalled due to
        switches from DSB to MITE pipelines]
  tma_info_botlnk_l2_dsb_misses
       [Total pipeline cost of DSB (uop cache) misses - subset of the
        Instruction_Fetch_BW Bottleneck]
  tma_info_frontend_dsb_switch_cost
       [Average number of cycles of a switch from the DSB fetch-unit to MITE
        fetch unit - see DSB_Switches tree node for details]
  tma_info_frontend_ipdsb_miss_ret
       [Instructions per non-speculative DSB miss (lower number means higher
        occurrence rate)]
  tma_mite
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to the MITE pipeline (the legacy decode pipeline)]

DataSharing: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_contested_accesses
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to contested accesses]
  tma_false_sharing
       [This metric roughly estimates how often CPU was handling
        synchronizations due to False Sharing]

Fed: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_botlnk_l2_dsb_misses
       [Total pipeline cost of DSB (uop cache) misses - subset of the
        Instruction_Fetch_BW Bottleneck]
  tma_info_botlnk_l2_ic_misses
       [Total pipeline cost of Instruction Cache misses - subset of the
        Big_Code Bottleneck]
  tma_info_bottleneck_big_code
       [Total pipeline cost of instruction fetch related bottlenecks by large
        code footprint programs (i-side cache; TLB and BTB misses)]
  tma_info_bottleneck_instruction_fetch_bw
       [Total pipeline cost of instruction fetch bandwidth related bottlenecks]
  tma_info_frontend_dsb_coverage
       [Fraction of Uops delivered by the DSB (aka Decoded ICache; or Uop
        Cache)]
  tma_info_frontend_fetch_upc
       [Average number of Uops issued by front-end when it issued something]
  tma_info_frontend_icache_miss_latency
       [Average Latency for L1 instruction cache misses]
  tma_info_frontend_ipdsb_miss_ret
       [Instructions per non-speculative DSB miss (lower number means higher
        occurrence rate)]
  tma_info_frontend_ipunknown_branch
       [Instructions per speculative Unknown Branch Misprediction (BAClear)
        (lower number means higher occurrence rate)]
  tma_info_inst_mix_bptkbranch
       [Branch instructions per taken branch]
  tma_info_inst_mix_ipbranch
       [Instructions per Branch (lower number means higher occurrence rate)]
  tma_info_inst_mix_ipcall
       [Instructions per (near) call (lower number means higher occurrence
        rate)]
  tma_info_inst_mix_iptb
       [Instruction per taken branch]
  tma_info_memory_tlb_code_stlb_mpki
       [STLB (2nd level TLB) code speculative misses per kilo instruction
        (misses of any page-size that complete the page walk)]
  tma_info_thread_uptb
       [Instruction per taken branch]

FetchBW: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_decoder0_alone
       [This metric represents fraction of cycles where decoder-0 was the only
        active decoder]
  tma_dsb
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to DSB (decoded uop cache) fetch pipeline]
  tma_fetch_bandwidth
       [This metric represents fraction of slots the CPU was stalled due to
        Frontend bandwidth issues]
  tma_info_bottleneck_instruction_fetch_bw
       [Total pipeline cost of instruction fetch bandwidth related bottlenecks]
  tma_info_frontend_dsb_coverage
       [Fraction of Uops delivered by the DSB (aka Decoded ICache; or Uop
        Cache)]
  tma_info_frontend_fetch_upc
       [Average number of Uops issued by front-end when it issued something]
  tma_info_inst_mix_iptb
       [Instruction per taken branch]
  tma_info_thread_uptb
       [Instruction per taken branch]
  tma_mite
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to the MITE pipeline (the legacy decode pipeline)]

FetchLat: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_branch_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers]
  tma_dsb_switches
       [This metric represents fraction of cycles the CPU was stalled due to
        switches from DSB to MITE pipelines]
  tma_icache_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        instruction cache misses]
  tma_info_botlnk_l2_ic_misses
       [Total pipeline cost of Instruction Cache misses - subset of the
        Big_Code Bottleneck]
  tma_info_frontend_icache_miss_latency
       [Average Latency for L1 instruction cache misses]
  tma_itlb_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        Instruction TLB (ITLB) misses]
  tma_lcp
       [This metric represents fraction of cycles CPU was stalled due to
        Length Changing Prefixes (LCPs)]
  tma_ms_switches
       [This metric estimates the fraction of cycles when the CPU was stalled
        due to switches of uop delivery to the Microcode Sequencer (MS)]
  tma_unknown_branches
       [This metric represents fraction of cycles the CPU was stalled due to
        new branch address clears]

Flops: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_fp_scalar
       [This metric approximates arithmetic floating-point (FP) scalar uops
        fraction the CPU has retired]
  tma_fp_vector
       [This metric approximates arithmetic floating-point (FP) vector uops
        fraction the CPU has retired aggregated across all vector widths]
  tma_fp_vector_128b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 128-bit wide vectors]
  tma_fp_vector_256b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 256-bit wide vectors]
  tma_info_core_flopc
       [Floating Point Operations Per Cycle]
  tma_info_core_fp_arith_utilization
       [Actual per-core usage of the Floating Point non-X87 execution units
        (regardless of precision or vector-width)]
  tma_info_inst_mix_iparith
       [Instructions per FP Arithmetic instruction (lower number means higher
        occurrence rate)]
  tma_info_inst_mix_iparith_avx128
       [Instructions per FP Arithmetic AVX/SSE 128-bit instruction (lower
        number means higher occurrence rate)]
  tma_info_inst_mix_iparith_avx256
       [Instructions per FP Arithmetic AVX* 256-bit instruction (lower number
        means higher occurrence rate)]
  tma_info_inst_mix_iparith_scalar_dp
       [Instructions per FP Arithmetic Scalar Double-Precision instruction
        (lower number means higher occurrence rate)]
  tma_info_inst_mix_iparith_scalar_sp
       [Instructions per FP Arithmetic Scalar Single-Precision instruction
        (lower number means higher occurrence rate)]
  tma_info_inst_mix_ipflop
       [Instructions per Floating Point (FP) Operation (lower number means
        higher occurrence rate)]
  tma_info_system_gflops
       [Giga Floating Point Operations Per Second]

FpScalar: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_inst_mix_iparith_scalar_dp
       [Instructions per FP Arithmetic Scalar Double-Precision instruction
        (lower number means higher occurrence rate)]
  tma_info_inst_mix_iparith_scalar_sp
       [Instructions per FP Arithmetic Scalar Single-Precision instruction
        (lower number means higher occurrence rate)]

FpVector: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_inst_mix_iparith_avx128
       [Instructions per FP Arithmetic AVX/SSE 128-bit instruction (lower
        number means higher occurrence rate)]
  tma_info_inst_mix_iparith_avx256
       [Instructions per FP Arithmetic AVX* 256-bit instruction (lower number
        means higher occurrence rate)]

Frontend: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_fetch_bandwidth
       [This metric represents fraction of slots the CPU was stalled due to
        Frontend bandwidth issues]
  tma_fetch_latency
       [This metric represents fraction of slots the CPU was stalled due to
        Frontend latency issues]
  tma_info_bottleneck_big_code
       [Total pipeline cost of instruction fetch related bottlenecks by large
        code footprint programs (i-side cache; TLB and BTB misses)]
  tma_info_bottleneck_instruction_fetch_bw
       [Total pipeline cost of instruction fetch bandwidth related bottlenecks]
  tma_info_inst_mix_iptb
       [Instruction per taken branch]

HPC: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_fp_arith
       [This metric represents overall arithmetic floating-point (FP)
        operations fraction the CPU has executed (retired)]
  tma_info_core_fp_arith_utilization
       [Actual per-core usage of the Floating Point non-X87 execution units
        (regardless of precision or vector-width)]
  tma_info_system_cpu_utilization
       [Average CPU Utilization]
  tma_info_system_dram_bw_use
       [Average external Memory Bandwidth Use for reads and writes [GB / sec]]
  tma_info_system_gflops
       [Giga Floating Point Operations Per Second]

IcMiss: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_icache_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        instruction cache misses]
  tma_info_botlnk_l2_ic_misses
       [Total pipeline cost of Instruction Cache misses - subset of the
        Big_Code Bottleneck]
  tma_info_bottleneck_big_code
       [Total pipeline cost of instruction fetch related bottlenecks by large
        code footprint programs (i-side cache; TLB and BTB misses)]
  tma_info_frontend_icache_miss_latency
       [Average Latency for L1 instruction cache misses]
  tma_info_frontend_l2mpki_code
       [L2 cache true code cacheline misses per kilo instruction]
  tma_info_frontend_l2mpki_code_all
       [L2 cache speculative code cacheline misses per kilo instruction]

InsType: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_inst_mix_iparith
       [Instructions per FP Arithmetic instruction (lower number means higher
        occurrence rate)]
  tma_info_inst_mix_iparith_avx128
       [Instructions per FP Arithmetic AVX/SSE 128-bit instruction (lower
        number means higher occurrence rate)]
  tma_info_inst_mix_iparith_avx256
       [Instructions per FP Arithmetic AVX* 256-bit instruction (lower number
        means higher occurrence rate)]
  tma_info_inst_mix_iparith_scalar_dp
       [Instructions per FP Arithmetic Scalar Double-Precision instruction
        (lower number means higher occurrence rate)]
  tma_info_inst_mix_iparith_scalar_sp
       [Instructions per FP Arithmetic Scalar Single-Precision instruction
        (lower number means higher occurrence rate)]
  tma_info_inst_mix_ipbranch
       [Instructions per Branch (lower number means higher occurrence rate)]
  tma_info_inst_mix_ipflop
       [Instructions per Floating Point (FP) Operation (lower number means
        higher occurrence rate)]
  tma_info_inst_mix_ipload
       [Instructions per Load (lower number means higher occurrence rate)]
  tma_info_inst_mix_ipstore
       [Instructions per Store (lower number means higher occurrence rate)]

MachineClears: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_clears_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Machine Clears]
  tma_machine_clears
       [This metric represents fraction of slots the CPU has wasted due to
        Machine Clears]

Mem: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_bottleneck_memory_bandwidth
       [Total pipeline cost of (external) Memory Bandwidth related bottlenecks]
  tma_info_bottleneck_memory_data_tlbs
       [Total pipeline cost of Memory Address Translation related bottlenecks
        (data-side TLBs)]
  tma_info_bottleneck_memory_latency
       [Total pipeline cost of Memory Latency related bottlenecks (external
        memory and off-core caches)]
  tma_info_memory_core_l1d_cache_fill_bw
       [Average per-core data fill bandwidth to the L1 data cache [GB / sec]]
  tma_info_memory_core_l2_cache_fill_bw
       [Average per-core data fill bandwidth to the L2 cache [GB / sec]]
  tma_info_memory_core_l3_cache_access_bw
       [Average per-core data access bandwidth to the L3 cache [GB / sec]]
  tma_info_memory_core_l3_cache_fill_bw
       [Average per-core data fill bandwidth to the L3 cache [GB / sec]]
  tma_info_memory_fb_hpki
       [Fill Buffer (FB) hits per kilo instructions for retired demand loads
        (L1D misses that merge into ongoing miss-handling entries)]
  tma_info_memory_l1mpki
       [L1 cache true misses per kilo instruction for retired demand loads]
  tma_info_memory_l1mpki_load
       [L1 cache true misses per kilo instruction for all demand loads
        (including speculative)]
  tma_info_memory_l2hpki_all
       [L2 cache hits per kilo instruction for all request types (including
        speculative)]
  tma_info_memory_l2hpki_load
       [L2 cache hits per kilo instruction for all demand loads (including
        speculative)]
  tma_info_memory_l2mpki
       [L2 cache true misses per kilo instruction for retired demand loads]
  tma_info_memory_l2mpki_all
       [L2 cache ([RKL+] true) misses per kilo instruction for all request
        types (including speculative)]
  tma_info_memory_l2mpki_load
       [L2 cache ([RKL+] true) misses per kilo instruction for all demand
        loads (including speculative)]
  tma_info_memory_l3mpki
       [L3 cache true misses per kilo instruction for retired demand loads]
  tma_info_memory_load_miss_real_latency
       [Actual Average Latency for L1 data-cache miss demand load operations
        (in core cycles)]
  tma_info_memory_mlp
       [Memory-Level-Parallelism (average number of L1 miss demand load when
        there is at least one such miss]
  tma_info_memory_thread_l1d_cache_fill_bw_1t
       [Average per-thread data fill bandwidth to the L1 data cache [GB / sec]]
  tma_info_memory_thread_l2_cache_fill_bw_1t
       [Average per-thread data fill bandwidth to the L2 cache [GB / sec]]
  tma_info_memory_thread_l3_cache_access_bw_1t
       [Average per-thread data access bandwidth to the L3 cache [GB / sec]]
  tma_info_memory_thread_l3_cache_fill_bw_1t
       [Average per-thread data fill bandwidth to the L3 cache [GB / sec]]
  tma_info_memory_tlb_load_stlb_mpki
       [STLB (2nd level TLB) data load speculative misses per kilo instruction
        (misses of any page-size that complete the page walk)]
  tma_info_memory_tlb_page_walks_utilization
       [Utilization of the core's Page Walker(s) serving STLB misses triggered
        by instruction/Load/Store accesses]
  tma_info_memory_tlb_store_stlb_mpki
       [STLB (2nd level TLB) data store speculative misses per kilo
        instruction (misses of any page-size that complete the page walk)]
  tma_info_system_dram_bw_use
       [Average external Memory Bandwidth Use for reads and writes [GB / sec]]
  tma_info_system_mem_parallel_reads
       [Average number of parallel data read requests to external memory]
  tma_info_system_mem_parallel_requests
       [Average number of parallel requests to external memory]
  tma_info_system_mem_read_latency
       [Average latency of data read request to external memory (in
        nanoseconds)]
  tma_info_system_mem_request_latency
       [Average latency of all requests to external memory (in Uncore cycles)]
  tma_info_thread_cpi
       [Cycles Per Instruction (per Logical Processor)]

MemoryBW: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_fb_full
       [This metric does a *rough estimation* of how often L1D Fill Buffer
        unavailability limited additional L1D miss memory access requests to
        proceed]
  tma_info_bottleneck_memory_bandwidth
       [Total pipeline cost of (external) Memory Bandwidth related bottlenecks]
  tma_info_memory_core_l1d_cache_fill_bw
       [Average per-core data fill bandwidth to the L1 data cache [GB / sec]]
  tma_info_memory_core_l2_cache_fill_bw
       [Average per-core data fill bandwidth to the L2 cache [GB / sec]]
  tma_info_memory_core_l3_cache_access_bw
       [Average per-core data access bandwidth to the L3 cache [GB / sec]]
  tma_info_memory_core_l3_cache_fill_bw
       [Average per-core data fill bandwidth to the L3 cache [GB / sec]]
  tma_info_memory_mlp
       [Memory-Level-Parallelism (average number of L1 miss demand load when
        there is at least one such miss]
  tma_info_memory_thread_l1d_cache_fill_bw_1t
       [Average per-thread data fill bandwidth to the L1 data cache [GB / sec]]
  tma_info_memory_thread_l2_cache_fill_bw_1t
       [Average per-thread data fill bandwidth to the L2 cache [GB / sec]]
  tma_info_memory_thread_l3_cache_access_bw_1t
       [Average per-thread data access bandwidth to the L3 cache [GB / sec]]
  tma_info_memory_thread_l3_cache_fill_bw_1t
       [Average per-thread data fill bandwidth to the L3 cache [GB / sec]]
  tma_info_system_dram_bw_use
       [Average external Memory Bandwidth Use for reads and writes [GB / sec]]
  tma_info_system_mem_parallel_reads
       [Average number of parallel data read requests to external memory]
  tma_mem_bandwidth
       [This metric estimates fraction of cycles where the core's performance
        was likely hurt due to approaching bandwidth limits of external memory
        (DRAM)]
  tma_sq_full
       [This metric measures fraction of cycles where the Super Queue (SQ) was
        full taking into account all request-types and both hardware SMT
        threads (Logical Processors)]

MemoryBound: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_dram_bound
       [This metric estimates how often the CPU was stalled on accesses to
        external memory (DRAM) by loads]
  tma_info_memory_load_miss_real_latency
       [Actual Average Latency for L1 data-cache miss demand load operations
        (in core cycles)]
  tma_info_memory_mlp
       [Memory-Level-Parallelism (average number of L1 miss demand load when
        there is at least one such miss]
  tma_l1_bound
       [This metric estimates how often the CPU was stalled without loads
        missing the L1 data cache]
  tma_l2_bound
       [This metric estimates how often the CPU was stalled due to L2 cache
        accesses by loads]
  tma_l3_bound
       [This metric estimates how often the CPU was stalled due to loads
        accesses to L3 cache or contended with a sibling Core]
  tma_store_bound
       [This metric estimates how often CPU was stalled due to RFO store
        memory accesses; RFO store issue a read-for-ownership request before
        the write]

MemoryLat: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_bottleneck_memory_latency
       [Total pipeline cost of Memory Latency related bottlenecks (external
        memory and off-core caches)]
  tma_info_memory_load_miss_real_latency
       [Actual Average Latency for L1 data-cache miss demand load operations
        (in core cycles)]
  tma_info_system_mem_read_latency
       [Average latency of data read request to external memory (in
        nanoseconds)]
  tma_l3_hit_latency
       [This metric represents fraction of cycles with demand load accesses
        that hit the L3 cache under unloaded scenarios (possibly L3 latency
        limited)]
  tma_mem_latency
       [This metric estimates fraction of cycles where the performance was
        likely hurt due to latency from external memory (DRAM)]
  tma_store_latency
       [This metric estimates fraction of cycles the CPU spent handling L1D
        store misses]

MemoryTLB: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_dtlb_load
       [This metric roughly estimates the fraction of cycles where the Data
        TLB (DTLB) was missed by load accesses]
  tma_dtlb_store
       [This metric roughly estimates the fraction of cycles spent handling
        first-level data TLB store misses]
  tma_info_bottleneck_big_code
       [Total pipeline cost of instruction fetch related bottlenecks by large
        code footprint programs (i-side cache; TLB and BTB misses)]
  tma_info_bottleneck_memory_data_tlbs
       [Total pipeline cost of Memory Address Translation related bottlenecks
        (data-side TLBs)]
  tma_info_memory_tlb_code_stlb_mpki
       [STLB (2nd level TLB) code speculative misses per kilo instruction
        (misses of any page-size that complete the page walk)]
  tma_info_memory_tlb_load_stlb_mpki
       [STLB (2nd level TLB) data load speculative misses per kilo instruction
        (misses of any page-size that complete the page walk)]
  tma_info_memory_tlb_page_walks_utilization
       [Utilization of the core's Page Walker(s) serving STLB misses triggered
        by instruction/Load/Store accesses]
  tma_info_memory_tlb_store_stlb_mpki
       [STLB (2nd level TLB) data store speculative misses per kilo
        instruction (misses of any page-size that complete the page walk)]
  tma_itlb_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        Instruction TLB (ITLB) misses]
  tma_load_stlb_hit
       [This metric roughly estimates the fraction of cycles where the (first
        level) DTLB was missed by load accesses, that later on hit in
        second-level TLB (STLB)]
  tma_load_stlb_miss
       [This metric estimates the fraction of cycles where the Second-level
        TLB (STLB) was missed by load accesses, performing a hardware page
        walk]
  tma_store_stlb_hit
       [This metric roughly estimates the fraction of cycles where the TLB was
        missed by store accesses, hitting in the second-level TLB (STLB)]
  tma_store_stlb_miss
       [This metric estimates the fraction of cycles where the STLB was missed
        by store accesses, performing a hardware page walk]

Memory_BW: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_memory_oro_data_l2_mlp
       [Average Parallel L2 cache miss data reads]
  tma_info_memory_oro_load_l2_mlp
       [Average Parallel L2 cache miss demand Loads]

Memory_Lat: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_memory_oro_load_l2_miss_latency
       [Average Latency for L2 cache miss demand Loads]

MicroSeq: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_microcode_sequencer
       [This metric represents fraction of slots the CPU was retiring uops
        fetched by the Microcode Sequencer (MS) unit]
  tma_ms_switches
       [This metric estimates the fraction of cycles when the CPU was stalled
        due to switches of uop delivery to the Microcode Sequencer (MS)]

OS: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_system_ipfarbranch
       [Instructions per Far Branch ( Far Branches apply upon transition from
        application to operating system, handling interrupts, exceptions)
        [lower number means higher occurrence rate]]
  tma_info_system_kernel_cpi
       [Cycles Per Instruction for the Operating System (OS) Kernel mode]
  tma_info_system_kernel_utilization
       [Fraction of cycles spent in the Operating System (OS) Kernel mode]

Offcore: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_contested_accesses
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to contested accesses]
  tma_data_sharing
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to data-sharing accesses]
  tma_false_sharing
       [This metric roughly estimates how often CPU was handling
        synchronizations due to False Sharing]
  tma_info_bottleneck_memory_bandwidth
       [Total pipeline cost of (external) Memory Bandwidth related bottlenecks]
  tma_info_bottleneck_memory_data_tlbs
       [Total pipeline cost of Memory Address Translation related bottlenecks
        (data-side TLBs)]
  tma_info_bottleneck_memory_latency
       [Total pipeline cost of Memory Latency related bottlenecks (external
        memory and off-core caches)]
  tma_info_memory_core_l3_cache_access_bw
       [Average per-core data access bandwidth to the L3 cache [GB / sec]]
  tma_info_memory_l2mpki_all
       [L2 cache ([RKL+] true) misses per kilo instruction for all request
        types (including speculative)]
  tma_info_memory_oro_data_l2_mlp
       [Average Parallel L2 cache miss data reads]
  tma_info_memory_oro_load_l2_miss_latency
       [Average Latency for L2 cache miss demand Loads]
  tma_info_memory_oro_load_l2_mlp
       [Average Parallel L2 cache miss demand Loads]
  tma_info_memory_thread_l3_cache_access_bw_1t
       [Average per-thread data access bandwidth to the L3 cache [GB / sec]]
  tma_lock_latency
       [This metric represents fraction of cycles the CPU spent handling cache
        misses due to lock operations]
  tma_mem_bandwidth
       [This metric estimates fraction of cycles where the core's performance
        was likely hurt due to approaching bandwidth limits of external memory
        (DRAM)]
  tma_mem_latency
       [This metric estimates fraction of cycles where the performance was
        likely hurt due to latency from external memory (DRAM)]
  tma_sq_full
       [This metric measures fraction of cycles where the Super Queue (SQ) was
        full taking into account all request-types and both hardware SMT
        threads (Logical Processors)]
  tma_store_latency
       [This metric estimates fraction of cycles the CPU spent handling L1D
        store misses]

PGO: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_frontend_bound
       [This category represents fraction of slots where the processor's
        Frontend undersupplies its Backend]
  tma_info_branches_cond_nt
       [Fraction of branches that are non-taken conditionals]
  tma_info_branches_cond_tk
       [Fraction of branches that are taken conditionals]
  tma_info_inst_mix_bptkbranch
       [Branch instructions per taken branch]
  tma_info_inst_mix_ipcall
       [Instructions per (near) call (lower number means higher occurrence
        rate)]
  tma_info_inst_mix_iptb
       [Instruction per taken branch]

Pipeline: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_fused_instructions
       [This metric represents fraction of slots where the CPU was retiring
        fused instructions -- where one uop can represent multiple contiguous
        instructions]
  tma_info_core_ilp
       [Instruction-Level-Parallelism (average number of uops executed when
        there is execution) per-core]
  tma_info_pipeline_execute
       [Instruction-Level-Parallelism (average number of uops executed when
        there is execution) per-thread]
  tma_info_pipeline_retire
       [Average number of Uops retired in cycles where at least one uop has
        retired]
  tma_info_thread_clks
       [Per-Logical Processor actual clocks when the Logical Processor is
        active]
  tma_info_thread_cpi
       [Cycles Per Instruction (per Logical Processor)]
  tma_info_thread_execute_per_issue
       [The ratio of Executed- by Issued-Uops]
  tma_info_thread_uoppi
       [Uops Per Instruction]
  tma_memory_operations
       [This metric represents fraction of slots where the CPU was retiring
        memory operations -- uops for memory load or store accesses]
  tma_non_fused_branches
       [This metric represents fraction of slots where the CPU was retiring
        branch instructions that were not fused]
  tma_nop_instructions
       [This metric represents fraction of slots where the CPU was retiring
        NOP (no op) instructions]
  tma_other_light_ops
       [This metric represents the remaining light uops fraction the CPU has
        executed - remaining means not covered by other sibling nodes]

PortsUtil: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_core_ilp
       [Instruction-Level-Parallelism (average number of uops executed when
        there is execution) per-core]
  tma_info_pipeline_execute
       [Instruction-Level-Parallelism (average number of uops executed when
        there is execution) per-thread]
  tma_ports_utilization
       [This metric estimates fraction of cycles the CPU performance was
        potentially limited due to Core computation issues (non
        divider-related)]
  tma_ports_utilized_0
       [This metric represents fraction of cycles CPU executed no uops on any
        execution port (Logical Processor cycles since ICL, Physical Core
        cycles otherwise)]
  tma_ports_utilized_1
       [This metric represents fraction of cycles where the CPU executed total
        of 1 uop per cycle on all execution ports (Logical Processor cycles
        since ICL, Physical Core cycles otherwise)]
  tma_ports_utilized_2
       [This metric represents fraction of cycles CPU executed total of 2 uops
        per cycle on all execution ports (Logical Processor cycles since ICL,
        Physical Core cycles otherwise)]
  tma_ports_utilized_3m
       [This metric represents fraction of cycles CPU executed total of 3 or
        more uops per cycle on all execution ports (Logical Processor cycles
        since ICL, Physical Core cycles otherwise)]
  tma_serializing_operation
       [This metric represents fraction of cycles the CPU issue-pipeline was
        stalled due to serializing operations]

Power: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  C2_Pkg_Residency
       [C2 residency percent per package]
  C3_Core_Residency
       [C3 residency percent per core]
  C3_Pkg_Residency
       [C3 residency percent per package]
  C6_Core_Residency
       [C6 residency percent per core]
  C6_Pkg_Residency
       [C6 residency percent per package]
  C7_Core_Residency
       [C7 residency percent per core]
  C7_Pkg_Residency
       [C7 residency percent per package]
  tma_info_system_average_frequency
       [Measured Average Frequency for unhalted processors [GHz]]
  tma_info_system_turbo_utilization
       [Average Frequency Utilization relative nominal frequency]

Prefetches: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_inst_mix_ipswpf
       [Instructions per Software prefetch instruction (of any type:
        NTA/T0/T1/T2/Prefetch) (lower number means higher occurrence rate)]

Ret: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_bottleneck_branching_overhead
       [Total pipeline cost of branch related instructions (used for program
        control-flow including function calls)]
  tma_info_core_coreipc
       [Instructions Per Cycle across hyper-threads (per physical core)]
  tma_info_core_flopc
       [Floating Point Operations Per Cycle]
  tma_info_pipeline_retire
       [Average number of Uops retired in cycles where at least one uop has
        retired]
  tma_info_thread_ipc
       [Instructions Per Cycle (per Logical Processor)]
  tma_info_thread_uoppi
       [Uops Per Instruction]

Retire: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_heavy_operations
       [This metric represents fraction of slots where the CPU was retiring
        heavy-weight operations -- instructions that require two or more uops
        or micro-coded sequences]
  tma_info_thread_uoppi
       [Uops Per Instruction]
  tma_light_operations
       [This metric represents fraction of slots where the CPU was retiring
        light-weight operations -- instructions that require no more than one
        uop (micro-operation)]

SMT: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_botlnk_l0_core_bound_likely
       [Probability of Core Bound bottleneck hidden by SMT-profiling artifacts]
  tma_info_core_core_clks
       [Core actual clocks when any Logical Processor is active on the
        Physical Core]
  tma_info_core_coreipc
       [Instructions Per Cycle across hyper-threads (per physical core)]
  tma_info_pipeline_execute
       [Instruction-Level-Parallelism (average number of uops executed when
        there is execution) per-thread]
  tma_info_system_smt_2t_utilization
       [Fraction of cycles where both hardware Logical Processors were active]

Snoop: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_contested_accesses
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to contested accesses]
  tma_data_sharing
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to data-sharing accesses]
  tma_false_sharing
       [This metric roughly estimates how often CPU was handling
        synchronizations due to False Sharing]

SoC: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  UNCORE_FREQ
       [Uncore frequency per die [GHZ]]
  tma_info_system_dram_bw_use
       [Average external Memory Bandwidth Use for reads and writes [GB / sec]]
  tma_info_system_mem_parallel_reads
       [Average number of parallel data read requests to external memory]
  tma_info_system_mem_parallel_requests
       [Average number of parallel requests to external memory]
  tma_info_system_mem_read_latency
       [Average latency of data read request to external memory (in
        nanoseconds)]
  tma_info_system_mem_request_latency
       [Average latency of all requests to external memory (in Uncore cycles)]
  tma_info_system_socket_clks
       [Socket actual clocks when any core is active on that socket]

Summary: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_inst_mix_instructions
       [Total number of retired Instructions]
  tma_info_system_average_frequency
       [Measured Average Frequency for unhalted processors [GHz]]
  tma_info_system_cpu_utilization
       [Average CPU Utilization]
  tma_info_thread_ipc
       [Instructions Per Cycle (per Logical Processor)]

TmaL1: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_backend_bound
       [This category represents fraction of slots where no uops are being
        delivered due to a lack of required resources for accepting new uops
        in the Backend]
  tma_bad_speculation
       [This category represents fraction of slots wasted due to incorrect
        speculations]
  tma_frontend_bound
       [This category represents fraction of slots where the processor's
        Frontend undersupplies its Backend]
  tma_info_core_coreipc
       [Instructions Per Cycle across hyper-threads (per physical core)]
  tma_info_inst_mix_instructions
       [Total number of retired Instructions]
  tma_info_thread_slots
       [Total issue-pipeline slots (per-Physical Core till ICL; per-Logical
        Processor ICL onward)]
  tma_retiring
       [This category represents fraction of slots utilized by useful work
        i.e. issued uops that eventually get retired]

TmaL2: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_branch_mispredicts
       [This metric represents fraction of slots the CPU has wasted due to
        Branch Misprediction]
  tma_core_bound
       [This metric represents fraction of slots where Core non-memory issues
        were of a bottleneck]
  tma_fetch_bandwidth
       [This metric represents fraction of slots the CPU was stalled due to
        Frontend bandwidth issues]
  tma_fetch_latency
       [This metric represents fraction of slots the CPU was stalled due to
        Frontend latency issues]
  tma_heavy_operations
       [This metric represents fraction of slots where the CPU was retiring
        heavy-weight operations -- instructions that require two or more uops
        or micro-coded sequences]
  tma_light_operations
       [This metric represents fraction of slots where the CPU was retiring
        light-weight operations -- instructions that require no more than one
        uop (micro-operation)]
  tma_machine_clears
       [This metric represents fraction of slots the CPU has wasted due to
        Machine Clears]
  tma_memory_bound
       [This metric represents fraction of slots the Memory subsystem within
        the Backend was a bottleneck]

TmaL3mem: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_dram_bound
       [This metric estimates how often the CPU was stalled on accesses to
        external memory (DRAM) by loads]
  tma_l1_bound
       [This metric estimates how often the CPU was stalled without loads
        missing the L1 data cache]
  tma_l2_bound
       [This metric estimates how often the CPU was stalled due to L2 cache
        accesses by loads]
  tma_l3_bound
       [This metric estimates how often the CPU was stalled due to loads
        accesses to L3 cache or contended with a sibling Core]
  tma_store_bound
       [This metric estimates how often CPU was stalled due to RFO store
        memory accesses; RFO store issue a read-for-ownership request before
        the write]

TopdownL1: [Metrics for top-down breakdown at level 1]
  tma_backend_bound
       [This category represents fraction of slots where no uops are being
        delivered due to a lack of required resources for accepting new uops
        in the Backend]
  tma_bad_speculation
       [This category represents fraction of slots wasted due to incorrect
        speculations]
  tma_frontend_bound
       [This category represents fraction of slots where the processor's
        Frontend undersupplies its Backend]
  tma_retiring
       [This category represents fraction of slots utilized by useful work
        i.e. issued uops that eventually get retired]

TopdownL2: [Metrics for top-down breakdown at level 2]
  tma_branch_mispredicts
       [This metric represents fraction of slots the CPU has wasted due to
        Branch Misprediction]
  tma_core_bound
       [This metric represents fraction of slots where Core non-memory issues
        were of a bottleneck]
  tma_fetch_bandwidth
       [This metric represents fraction of slots the CPU was stalled due to
        Frontend bandwidth issues]
  tma_fetch_latency
       [This metric represents fraction of slots the CPU was stalled due to
        Frontend latency issues]
  tma_heavy_operations
       [This metric represents fraction of slots where the CPU was retiring
        heavy-weight operations -- instructions that require two or more uops
        or micro-coded sequences]
  tma_light_operations
       [This metric represents fraction of slots where the CPU was retiring
        light-weight operations -- instructions that require no more than one
        uop (micro-operation)]
  tma_machine_clears
       [This metric represents fraction of slots the CPU has wasted due to
        Machine Clears]
  tma_memory_bound
       [This metric represents fraction of slots the Memory subsystem within
        the Backend was a bottleneck]

TopdownL3: [Metrics for top-down breakdown at level 3]
  tma_branch_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers]
  tma_divider
       [This metric represents fraction of cycles where the Divider unit was
        active]
  tma_dram_bound
       [This metric estimates how often the CPU was stalled on accesses to
        external memory (DRAM) by loads]
  tma_dsb
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to DSB (decoded uop cache) fetch pipeline]
  tma_dsb_switches
       [This metric represents fraction of cycles the CPU was stalled due to
        switches from DSB to MITE pipelines]
  tma_few_uops_instructions
       [This metric represents fraction of slots where the CPU was retiring
        instructions that that are decoder into two or up to ([SNB+] four;
        [ADL+] five) uops]
  tma_fp_arith
       [This metric represents overall arithmetic floating-point (FP)
        operations fraction the CPU has executed (retired)]
  tma_fused_instructions
       [This metric represents fraction of slots where the CPU was retiring
        fused instructions -- where one uop can represent multiple contiguous
        instructions]
  tma_icache_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        instruction cache misses]
  tma_itlb_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        Instruction TLB (ITLB) misses]
  tma_l1_bound
       [This metric estimates how often the CPU was stalled without loads
        missing the L1 data cache]
  tma_l2_bound
       [This metric estimates how often the CPU was stalled due to L2 cache
        accesses by loads]
  tma_l3_bound
       [This metric estimates how often the CPU was stalled due to loads
        accesses to L3 cache or contended with a sibling Core]
  tma_lcp
       [This metric represents fraction of cycles CPU was stalled due to
        Length Changing Prefixes (LCPs)]
  tma_memory_operations
       [This metric represents fraction of slots where the CPU was retiring
        memory operations -- uops for memory load or store accesses]
  tma_microcode_sequencer
       [This metric represents fraction of slots the CPU was retiring uops
        fetched by the Microcode Sequencer (MS) unit]
  tma_mite
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to the MITE pipeline (the legacy decode pipeline)]
  tma_ms_switches
       [This metric estimates the fraction of cycles when the CPU was stalled
        due to switches of uop delivery to the Microcode Sequencer (MS)]
  tma_non_fused_branches
       [This metric represents fraction of slots where the CPU was retiring
        branch instructions that were not fused]
  tma_nop_instructions
       [This metric represents fraction of slots where the CPU was retiring
        NOP (no op) instructions]
  tma_other_light_ops
       [This metric represents the remaining light uops fraction the CPU has
        executed - remaining means not covered by other sibling nodes]
  tma_ports_utilization
       [This metric estimates fraction of cycles the CPU performance was
        potentially limited due to Core computation issues (non
        divider-related)]
  tma_store_bound
       [This metric estimates how often CPU was stalled due to RFO store
        memory accesses; RFO store issue a read-for-ownership request before
        the write]

TopdownL4: [Metrics for top-down breakdown at level 4]
  tma_4k_aliasing
       [This metric estimates how often memory load accesses were aliased by
        preceding stores (in program order) with a 4K address offset]
  tma_assists
       [This metric estimates fraction of slots the CPU retired uops delivered
        by the Microcode_Sequencer as a result of Assists]
  tma_cisc
       [This metric estimates fraction of cycles the CPU retired uops
        originated from CISC (complex instruction set computer) instruction]
  tma_clears_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Machine Clears]
  tma_contested_accesses
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to contested accesses]
  tma_data_sharing
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to data-sharing accesses]
  tma_decoder0_alone
       [This metric represents fraction of cycles where decoder-0 was the only
        active decoder]
  tma_dtlb_load
       [This metric roughly estimates the fraction of cycles where the Data
        TLB (DTLB) was missed by load accesses]
  tma_dtlb_store
       [This metric roughly estimates the fraction of cycles spent handling
        first-level data TLB store misses]
  tma_false_sharing
       [This metric roughly estimates how often CPU was handling
        synchronizations due to False Sharing]
  tma_fb_full
       [This metric does a *rough estimation* of how often L1D Fill Buffer
        unavailability limited additional L1D miss memory access requests to
        proceed]
  tma_fp_scalar
       [This metric approximates arithmetic floating-point (FP) scalar uops
        fraction the CPU has retired]
  tma_fp_vector
       [This metric approximates arithmetic floating-point (FP) vector uops
        fraction the CPU has retired aggregated across all vector widths]
  tma_l3_hit_latency
       [This metric represents fraction of cycles with demand load accesses
        that hit the L3 cache under unloaded scenarios (possibly L3 latency
        limited)]
  tma_lock_latency
       [This metric represents fraction of cycles the CPU spent handling cache
        misses due to lock operations]
  tma_mem_bandwidth
       [This metric estimates fraction of cycles where the core's performance
        was likely hurt due to approaching bandwidth limits of external memory
        (DRAM)]
  tma_mem_latency
       [This metric estimates fraction of cycles where the performance was
        likely hurt due to latency from external memory (DRAM)]
  tma_mispredicts_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Branch Misprediction at execution stage]
  tma_ports_utilized_0
       [This metric represents fraction of cycles CPU executed no uops on any
        execution port (Logical Processor cycles since ICL, Physical Core
        cycles otherwise)]
  tma_ports_utilized_1
       [This metric represents fraction of cycles where the CPU executed total
        of 1 uop per cycle on all execution ports (Logical Processor cycles
        since ICL, Physical Core cycles otherwise)]
  tma_ports_utilized_2
       [This metric represents fraction of cycles CPU executed total of 2 uops
        per cycle on all execution ports (Logical Processor cycles since ICL,
        Physical Core cycles otherwise)]
  tma_ports_utilized_3m
       [This metric represents fraction of cycles CPU executed total of 3 or
        more uops per cycle on all execution ports (Logical Processor cycles
        since ICL, Physical Core cycles otherwise)]
  tma_split_loads
       [This metric estimates fraction of cycles handling memory load split
        accesses - load that cross 64-byte cache line boundary]
  tma_split_stores
       [This metric represents rate of split store accesses]
  tma_sq_full
       [This metric measures fraction of cycles where the Super Queue (SQ) was
        full taking into account all request-types and both hardware SMT
        threads (Logical Processors)]
  tma_store_fwd_blk
       [This metric roughly estimates fraction of cycles when the memory
        subsystem had loads blocked since they could not forward data from
        earlier (in program order) overlapping stores]
  tma_store_latency
       [This metric estimates fraction of cycles the CPU spent handling L1D
        store misses]
  tma_unknown_branches
       [This metric represents fraction of cycles the CPU was stalled due to
        new branch address clears]
  tma_x87_use
       [This metric serves as an approximation of legacy x87 usage]

TopdownL5: [Metrics for top-down breakdown at level 5]
  tma_alu_op_utilization
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution ports for ALU operations]
  tma_fp_vector_128b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 128-bit wide vectors]
  tma_fp_vector_256b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 256-bit wide vectors]
  tma_load_op_utilization
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port for Load operations]
  tma_load_stlb_hit
       [This metric roughly estimates the fraction of cycles where the (first
        level) DTLB was missed by load accesses, that later on hit in
        second-level TLB (STLB)]
  tma_load_stlb_miss
       [This metric estimates the fraction of cycles where the Second-level
        TLB (STLB) was missed by load accesses, performing a hardware page
        walk]
  tma_mixing_vectors
       [The Mixing_Vectors metric gives the percentage of injected blend uops
        out of all uops issued]
  tma_serializing_operation
       [This metric represents fraction of cycles the CPU issue-pipeline was
        stalled due to serializing operations]
  tma_store_op_utilization
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port for Store operations]
  tma_store_stlb_hit
       [This metric roughly estimates the fraction of cycles where the TLB was
        missed by store accesses, hitting in the second-level TLB (STLB)]
  tma_store_stlb_miss
       [This metric estimates the fraction of cycles where the STLB was missed
        by store accesses, performing a hardware page walk]

TopdownL6: [Metrics for top-down breakdown at level 6]
  tma_port_0
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 0 ([SNB+] ALU; [HSW+] ALU and 2nd branch)]
  tma_port_1
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 1 (ALU)]
  tma_port_2
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 2 ([SNB+]Loads and Store-address; [ICL+] Loads)]
  tma_port_3
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 3 ([SNB+]Loads and Store-address; [ICL+] Loads)]
  tma_port_4
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 4 (Store-data)]
  tma_port_5
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 5 ([SNB+] Branches and ALU; [HSW+] ALU)]
  tma_port_6
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 6 ([HSW+]Primary Branch and simple ALU)]
  tma_port_7
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 7 ([HSW+]simple Store-address)]

smi:
  smi_cycles
       [Percentage of cycles spent in System Management Interrupts]
  smi_num
       [Number of SMI interrupts]

tma_L1_group: [Metrics for top-down breakdown at level 1]
  tma_backend_bound
       [This category represents fraction of slots where no uops are being
        delivered due to a lack of required resources for accepting new uops
        in the Backend]
  tma_bad_speculation
       [This category represents fraction of slots wasted due to incorrect
        speculations]
  tma_frontend_bound
       [This category represents fraction of slots where the processor's
        Frontend undersupplies its Backend]
  tma_info_core_coreipc
       [Instructions Per Cycle across hyper-threads (per physical core)]
  tma_info_inst_mix_instructions
       [Total number of retired Instructions]
  tma_info_thread_slots
       [Total issue-pipeline slots (per-Physical Core till ICL; per-Logical
        Processor ICL onward)]
  tma_retiring
       [This category represents fraction of slots utilized by useful work
        i.e. issued uops that eventually get retired]

tma_L2_group: [Metrics for top-down breakdown at level 2]
  tma_branch_mispredicts
       [This metric represents fraction of slots the CPU has wasted due to
        Branch Misprediction]
  tma_core_bound
       [This metric represents fraction of slots where Core non-memory issues
        were of a bottleneck]
  tma_fetch_bandwidth
       [This metric represents fraction of slots the CPU was stalled due to
        Frontend bandwidth issues]
  tma_fetch_latency
       [This metric represents fraction of slots the CPU was stalled due to
        Frontend latency issues]
  tma_heavy_operations
       [This metric represents fraction of slots where the CPU was retiring
        heavy-weight operations -- instructions that require two or more uops
        or micro-coded sequences]
  tma_light_operations
       [This metric represents fraction of slots where the CPU was retiring
        light-weight operations -- instructions that require no more than one
        uop (micro-operation)]
  tma_machine_clears
       [This metric represents fraction of slots the CPU has wasted due to
        Machine Clears]
  tma_memory_bound
       [This metric represents fraction of slots the Memory subsystem within
        the Backend was a bottleneck]

tma_L3_group: [Metrics for top-down breakdown at level 3]
  tma_branch_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers]
  tma_divider
       [This metric represents fraction of cycles where the Divider unit was
        active]
  tma_dram_bound
       [This metric estimates how often the CPU was stalled on accesses to
        external memory (DRAM) by loads]
  tma_dsb
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to DSB (decoded uop cache) fetch pipeline]
  tma_dsb_switches
       [This metric represents fraction of cycles the CPU was stalled due to
        switches from DSB to MITE pipelines]
  tma_few_uops_instructions
       [This metric represents fraction of slots where the CPU was retiring
        instructions that that are decoder into two or up to ([SNB+] four;
        [ADL+] five) uops]
  tma_fp_arith
       [This metric represents overall arithmetic floating-point (FP)
        operations fraction the CPU has executed (retired)]
  tma_fused_instructions
       [This metric represents fraction of slots where the CPU was retiring
        fused instructions -- where one uop can represent multiple contiguous
        instructions]
  tma_icache_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        instruction cache misses]
  tma_itlb_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        Instruction TLB (ITLB) misses]
  tma_l1_bound
       [This metric estimates how often the CPU was stalled without loads
        missing the L1 data cache]
  tma_l2_bound
       [This metric estimates how often the CPU was stalled due to L2 cache
        accesses by loads]
  tma_l3_bound
       [This metric estimates how often the CPU was stalled due to loads
        accesses to L3 cache or contended with a sibling Core]
  tma_lcp
       [This metric represents fraction of cycles CPU was stalled due to
        Length Changing Prefixes (LCPs)]
  tma_memory_operations
       [This metric represents fraction of slots where the CPU was retiring
        memory operations -- uops for memory load or store accesses]
  tma_microcode_sequencer
       [This metric represents fraction of slots the CPU was retiring uops
        fetched by the Microcode Sequencer (MS) unit]
  tma_mite
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to the MITE pipeline (the legacy decode pipeline)]
  tma_ms_switches
       [This metric estimates the fraction of cycles when the CPU was stalled
        due to switches of uop delivery to the Microcode Sequencer (MS)]
  tma_non_fused_branches
       [This metric represents fraction of slots where the CPU was retiring
        branch instructions that were not fused]
  tma_nop_instructions
       [This metric represents fraction of slots where the CPU was retiring
        NOP (no op) instructions]
  tma_other_light_ops
       [This metric represents the remaining light uops fraction the CPU has
        executed - remaining means not covered by other sibling nodes]
  tma_ports_utilization
       [This metric estimates fraction of cycles the CPU performance was
        potentially limited due to Core computation issues (non
        divider-related)]
  tma_store_bound
       [This metric estimates how often CPU was stalled due to RFO store
        memory accesses; RFO store issue a read-for-ownership request before
        the write]

tma_L4_group: [Metrics for top-down breakdown at level 4]
  tma_4k_aliasing
       [This metric estimates how often memory load accesses were aliased by
        preceding stores (in program order) with a 4K address offset]
  tma_assists
       [This metric estimates fraction of slots the CPU retired uops delivered
        by the Microcode_Sequencer as a result of Assists]
  tma_cisc
       [This metric estimates fraction of cycles the CPU retired uops
        originated from CISC (complex instruction set computer) instruction]
  tma_clears_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Machine Clears]
  tma_contested_accesses
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to contested accesses]
  tma_data_sharing
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to data-sharing accesses]
  tma_decoder0_alone
       [This metric represents fraction of cycles where decoder-0 was the only
        active decoder]
  tma_dtlb_load
       [This metric roughly estimates the fraction of cycles where the Data
        TLB (DTLB) was missed by load accesses]
  tma_dtlb_store
       [This metric roughly estimates the fraction of cycles spent handling
        first-level data TLB store misses]
  tma_false_sharing
       [This metric roughly estimates how often CPU was handling
        synchronizations due to False Sharing]
  tma_fb_full
       [This metric does a *rough estimation* of how often L1D Fill Buffer
        unavailability limited additional L1D miss memory access requests to
        proceed]
  tma_fp_scalar
       [This metric approximates arithmetic floating-point (FP) scalar uops
        fraction the CPU has retired]
  tma_fp_vector
       [This metric approximates arithmetic floating-point (FP) vector uops
        fraction the CPU has retired aggregated across all vector widths]
  tma_l3_hit_latency
       [This metric represents fraction of cycles with demand load accesses
        that hit the L3 cache under unloaded scenarios (possibly L3 latency
        limited)]
  tma_lock_latency
       [This metric represents fraction of cycles the CPU spent handling cache
        misses due to lock operations]
  tma_mem_bandwidth
       [This metric estimates fraction of cycles where the core's performance
        was likely hurt due to approaching bandwidth limits of external memory
        (DRAM)]
  tma_mem_latency
       [This metric estimates fraction of cycles where the performance was
        likely hurt due to latency from external memory (DRAM)]
  tma_mispredicts_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Branch Misprediction at execution stage]
  tma_ports_utilized_0
       [This metric represents fraction of cycles CPU executed no uops on any
        execution port (Logical Processor cycles since ICL, Physical Core
        cycles otherwise)]
  tma_ports_utilized_1
       [This metric represents fraction of cycles where the CPU executed total
        of 1 uop per cycle on all execution ports (Logical Processor cycles
        since ICL, Physical Core cycles otherwise)]
  tma_ports_utilized_2
       [This metric represents fraction of cycles CPU executed total of 2 uops
        per cycle on all execution ports (Logical Processor cycles since ICL,
        Physical Core cycles otherwise)]
  tma_ports_utilized_3m
       [This metric represents fraction of cycles CPU executed total of 3 or
        more uops per cycle on all execution ports (Logical Processor cycles
        since ICL, Physical Core cycles otherwise)]
  tma_split_loads
       [This metric estimates fraction of cycles handling memory load split
        accesses - load that cross 64-byte cache line boundary]
  tma_split_stores
       [This metric represents rate of split store accesses]
  tma_sq_full
       [This metric measures fraction of cycles where the Super Queue (SQ) was
        full taking into account all request-types and both hardware SMT
        threads (Logical Processors)]
  tma_store_fwd_blk
       [This metric roughly estimates fraction of cycles when the memory
        subsystem had loads blocked since they could not forward data from
        earlier (in program order) overlapping stores]
  tma_store_latency
       [This metric estimates fraction of cycles the CPU spent handling L1D
        store misses]
  tma_unknown_branches
       [This metric represents fraction of cycles the CPU was stalled due to
        new branch address clears]
  tma_x87_use
       [This metric serves as an approximation of legacy x87 usage]

tma_L5_group: [Metrics for top-down breakdown at level 5]
  tma_alu_op_utilization
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution ports for ALU operations]
  tma_fp_vector_128b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 128-bit wide vectors]
  tma_fp_vector_256b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 256-bit wide vectors]
  tma_load_op_utilization
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port for Load operations]
  tma_load_stlb_hit
       [This metric roughly estimates the fraction of cycles where the (first
        level) DTLB was missed by load accesses, that later on hit in
        second-level TLB (STLB)]
  tma_load_stlb_miss
       [This metric estimates the fraction of cycles where the Second-level
        TLB (STLB) was missed by load accesses, performing a hardware page
        walk]
  tma_mixing_vectors
       [The Mixing_Vectors metric gives the percentage of injected blend uops
        out of all uops issued]
  tma_serializing_operation
       [This metric represents fraction of cycles the CPU issue-pipeline was
        stalled due to serializing operations]
  tma_store_op_utilization
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port for Store operations]
  tma_store_stlb_hit
       [This metric roughly estimates the fraction of cycles where the TLB was
        missed by store accesses, hitting in the second-level TLB (STLB)]
  tma_store_stlb_miss
       [This metric estimates the fraction of cycles where the STLB was missed
        by store accesses, performing a hardware page walk]

tma_L6_group: [Metrics for top-down breakdown at level 6]
  tma_port_0
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 0 ([SNB+] ALU; [HSW+] ALU and 2nd branch)]
  tma_port_1
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 1 (ALU)]
  tma_port_2
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 2 ([SNB+]Loads and Store-address; [ICL+] Loads)]
  tma_port_3
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 3 ([SNB+]Loads and Store-address; [ICL+] Loads)]
  tma_port_4
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 4 (Store-data)]
  tma_port_5
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 5 ([SNB+] Branches and ALU; [HSW+] ALU)]
  tma_port_6
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 6 ([HSW+]Primary Branch and simple ALU)]
  tma_port_7
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 7 ([HSW+]simple Store-address)]

tma_alu_op_utilization_group: [Metrics contributing to tma_alu_op_utilization category]
  tma_port_0
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 0 ([SNB+] ALU; [HSW+] ALU and 2nd branch)]
  tma_port_1
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 1 (ALU)]
  tma_port_5
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 5 ([SNB+] Branches and ALU; [HSW+] ALU)]
  tma_port_6
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 6 ([HSW+]Primary Branch and simple ALU)]

tma_backend_bound_group: [Metrics contributing to tma_backend_bound category]
  tma_core_bound
       [This metric represents fraction of slots where Core non-memory issues
        were of a bottleneck]
  tma_memory_bound
       [This metric represents fraction of slots the Memory subsystem within
        the Backend was a bottleneck]

tma_bad_speculation_group: [Metrics contributing to tma_bad_speculation category]
  tma_branch_mispredicts
       [This metric represents fraction of slots the CPU has wasted due to
        Branch Misprediction]
  tma_machine_clears
       [This metric represents fraction of slots the CPU has wasted due to
        Machine Clears]

tma_branch_resteers_group: [Metrics contributing to tma_branch_resteers category]
  tma_clears_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Machine Clears]
  tma_mispredicts_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Branch Misprediction at execution stage]
  tma_unknown_branches
       [This metric represents fraction of cycles the CPU was stalled due to
        new branch address clears]

tma_core_bound_group: [Metrics contributing to tma_core_bound category]
  tma_divider
       [This metric represents fraction of cycles where the Divider unit was
        active]
  tma_ports_utilization
       [This metric estimates fraction of cycles the CPU performance was
        potentially limited due to Core computation issues (non
        divider-related)]

tma_dram_bound_group: [Metrics contributing to tma_dram_bound category]
  tma_mem_bandwidth
       [This metric estimates fraction of cycles where the core's performance
        was likely hurt due to approaching bandwidth limits of external memory
        (DRAM)]
  tma_mem_latency
       [This metric estimates fraction of cycles where the performance was
        likely hurt due to latency from external memory (DRAM)]

tma_dtlb_load_group: [Metrics contributing to tma_dtlb_load category]
  tma_load_stlb_hit
       [This metric roughly estimates the fraction of cycles where the (first
        level) DTLB was missed by load accesses, that later on hit in
        second-level TLB (STLB)]
  tma_load_stlb_miss
       [This metric estimates the fraction of cycles where the Second-level
        TLB (STLB) was missed by load accesses, performing a hardware page
        walk]

tma_dtlb_store_group: [Metrics contributing to tma_dtlb_store category]
  tma_store_stlb_hit
       [This metric roughly estimates the fraction of cycles where the TLB was
        missed by store accesses, hitting in the second-level TLB (STLB)]
  tma_store_stlb_miss
       [This metric estimates the fraction of cycles where the STLB was missed
        by store accesses, performing a hardware page walk]

tma_fetch_bandwidth_group: [Metrics contributing to tma_fetch_bandwidth category]
  tma_dsb
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to DSB (decoded uop cache) fetch pipeline]
  tma_mite
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to the MITE pipeline (the legacy decode pipeline)]

tma_fetch_latency_group: [Metrics contributing to tma_fetch_latency category]
  tma_branch_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers]
  tma_dsb_switches
       [This metric represents fraction of cycles the CPU was stalled due to
        switches from DSB to MITE pipelines]
  tma_icache_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        instruction cache misses]
  tma_itlb_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        Instruction TLB (ITLB) misses]
  tma_lcp
       [This metric represents fraction of cycles CPU was stalled due to
        Length Changing Prefixes (LCPs)]
  tma_ms_switches
       [This metric estimates the fraction of cycles when the CPU was stalled
        due to switches of uop delivery to the Microcode Sequencer (MS)]

tma_fp_arith_group: [Metrics contributing to tma_fp_arith category]
  tma_fp_scalar
       [This metric approximates arithmetic floating-point (FP) scalar uops
        fraction the CPU has retired]
  tma_fp_vector
       [This metric approximates arithmetic floating-point (FP) vector uops
        fraction the CPU has retired aggregated across all vector widths]
  tma_x87_use
       [This metric serves as an approximation of legacy x87 usage]

tma_fp_vector_group: [Metrics contributing to tma_fp_vector category]
  tma_fp_vector_128b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 128-bit wide vectors]
  tma_fp_vector_256b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 256-bit wide vectors]

tma_frontend_bound_group: [Metrics contributing to tma_frontend_bound category]
  tma_fetch_bandwidth
       [This metric represents fraction of slots the CPU was stalled due to
        Frontend bandwidth issues]
  tma_fetch_latency
       [This metric represents fraction of slots the CPU was stalled due to
        Frontend latency issues]

tma_heavy_operations_group: [Metrics contributing to tma_heavy_operations category]
  tma_few_uops_instructions
       [This metric represents fraction of slots where the CPU was retiring
        instructions that that are decoder into two or up to ([SNB+] four;
        [ADL+] five) uops]
  tma_microcode_sequencer
       [This metric represents fraction of slots the CPU was retiring uops
        fetched by the Microcode Sequencer (MS) unit]

tma_issue2P: [Metrics related by the issue $issue2P]
  tma_fp_scalar
       [This metric approximates arithmetic floating-point (FP) scalar uops
        fraction the CPU has retired]
  tma_fp_vector
       [This metric approximates arithmetic floating-point (FP) vector uops
        fraction the CPU has retired aggregated across all vector widths]
  tma_fp_vector_128b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 128-bit wide vectors]
  tma_fp_vector_256b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 256-bit wide vectors]
  tma_port_0
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 0 ([SNB+] ALU; [HSW+] ALU and 2nd branch)]
  tma_port_1
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 1 (ALU)]
  tma_port_5
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 5 ([SNB+] Branches and ALU; [HSW+] ALU)]
  tma_port_6
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 6 ([HSW+]Primary Branch and simple ALU)]
  tma_ports_utilized_2
       [This metric represents fraction of cycles CPU executed total of 2 uops
        per cycle on all execution ports (Logical Processor cycles since ICL,
        Physical Core cycles otherwise)]

tma_issueBC: [Metrics related by the issue $issueBC]
  tma_info_bottleneck_big_code
       [Total pipeline cost of instruction fetch related bottlenecks by large
        code footprint programs (i-side cache; TLB and BTB misses)]
  tma_info_bottleneck_branching_overhead
       [Total pipeline cost of branch related instructions (used for program
        control-flow including function calls)]

tma_issueBM: [Metrics related by the issue $issueBM]
  tma_branch_mispredicts
       [This metric represents fraction of slots the CPU has wasted due to
        Branch Misprediction]
  tma_info_bad_spec_branch_misprediction_cost
       [Branch Misprediction Cost: Fraction of TMA slots wasted per
        non-speculative branch misprediction (retired JEClear)]
  tma_info_bottleneck_mispredictions
       [Total pipeline cost of Branch Misprediction related bottlenecks]
  tma_mispredicts_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Branch Misprediction at execution stage]

tma_issueBW: [Metrics related by the issue $issueBW]
  tma_fb_full
       [This metric does a *rough estimation* of how often L1D Fill Buffer
        unavailability limited additional L1D miss memory access requests to
        proceed]
  tma_info_bottleneck_memory_bandwidth
       [Total pipeline cost of (external) Memory Bandwidth related bottlenecks]
  tma_info_system_dram_bw_use
       [Average external Memory Bandwidth Use for reads and writes [GB / sec]]
  tma_mem_bandwidth
       [This metric estimates fraction of cycles where the core's performance
        was likely hurt due to approaching bandwidth limits of external memory
        (DRAM)]
  tma_sq_full
       [This metric measures fraction of cycles where the Super Queue (SQ) was
        full taking into account all request-types and both hardware SMT
        threads (Logical Processors)]

tma_issueD0: [Metrics related by the issue $issueD0]
  tma_decoder0_alone
       [This metric represents fraction of cycles where decoder-0 was the only
        active decoder]
  tma_few_uops_instructions
       [This metric represents fraction of slots where the CPU was retiring
        instructions that that are decoder into two or up to ([SNB+] four;
        [ADL+] five) uops]

tma_issueFB: [Metrics related by the issue $issueFB]
  tma_dsb_switches
       [This metric represents fraction of cycles the CPU was stalled due to
        switches from DSB to MITE pipelines]
  tma_fetch_bandwidth
       [This metric represents fraction of slots the CPU was stalled due to
        Frontend bandwidth issues]
  tma_info_botlnk_l2_dsb_misses
       [Total pipeline cost of DSB (uop cache) misses - subset of the
        Instruction_Fetch_BW Bottleneck]
  tma_info_frontend_dsb_coverage
       [Fraction of Uops delivered by the DSB (aka Decoded ICache; or Uop
        Cache)]
  tma_info_inst_mix_iptb
       [Instruction per taken branch]
  tma_lcp
       [This metric represents fraction of cycles CPU was stalled due to
        Length Changing Prefixes (LCPs)]

tma_issueFL: [Metrics related by the issue $issueFL]
  tma_info_botlnk_l2_ic_misses
       [Total pipeline cost of Instruction Cache misses - subset of the
        Big_Code Bottleneck]

tma_issueL1: [Metrics related by the issue $issueL1]
  tma_l1_bound
       [This metric estimates how often the CPU was stalled without loads
        missing the L1 data cache]
  tma_ports_utilized_1
       [This metric represents fraction of cycles where the CPU executed total
        of 1 uop per cycle on all execution ports (Logical Processor cycles
        since ICL, Physical Core cycles otherwise)]

tma_issueLat: [Metrics related by the issue $issueLat]
  tma_info_bottleneck_memory_latency
       [Total pipeline cost of Memory Latency related bottlenecks (external
        memory and off-core caches)]
  tma_l3_hit_latency
       [This metric represents fraction of cycles with demand load accesses
        that hit the L3 cache under unloaded scenarios (possibly L3 latency
        limited)]
  tma_mem_latency
       [This metric estimates fraction of cycles where the performance was
        likely hurt due to latency from external memory (DRAM)]

tma_issueMC: [Metrics related by the issue $issueMC]
  tma_clears_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Machine Clears]
  tma_l1_bound
       [This metric estimates how often the CPU was stalled without loads
        missing the L1 data cache]
  tma_machine_clears
       [This metric represents fraction of slots the CPU has wasted due to
        Machine Clears]
  tma_microcode_sequencer
       [This metric represents fraction of slots the CPU was retiring uops
        fetched by the Microcode Sequencer (MS) unit]
  tma_ms_switches
       [This metric estimates the fraction of cycles when the CPU was stalled
        due to switches of uop delivery to the Microcode Sequencer (MS)]

tma_issueMS: [Metrics related by the issue $issueMS]
  tma_microcode_sequencer
       [This metric represents fraction of slots the CPU was retiring uops
        fetched by the Microcode Sequencer (MS) unit]
  tma_ms_switches
       [This metric estimates the fraction of cycles when the CPU was stalled
        due to switches of uop delivery to the Microcode Sequencer (MS)]

tma_issueMV: [Metrics related by the issue $issueMV]
  tma_mixing_vectors
       [The Mixing_Vectors metric gives the percentage of injected blend uops
        out of all uops issued]
  tma_ms_switches
       [This metric estimates the fraction of cycles when the CPU was stalled
        due to switches of uop delivery to the Microcode Sequencer (MS)]

tma_issueRFO: [Metrics related by the issue $issueRFO]
  tma_lock_latency
       [This metric represents fraction of cycles the CPU spent handling cache
        misses due to lock operations]
  tma_store_latency
       [This metric estimates fraction of cycles the CPU spent handling L1D
        store misses]

tma_issueSL: [Metrics related by the issue $issueSL]
  tma_fb_full
       [This metric does a *rough estimation* of how often L1D Fill Buffer
        unavailability limited additional L1D miss memory access requests to
        proceed]
  tma_store_latency
       [This metric estimates fraction of cycles the CPU spent handling L1D
        store misses]

tma_issueSO: [Metrics related by the issue $issueSO]
  tma_ms_switches
       [This metric estimates the fraction of cycles when the CPU was stalled
        due to switches of uop delivery to the Microcode Sequencer (MS)]
  tma_serializing_operation
       [This metric represents fraction of cycles the CPU issue-pipeline was
        stalled due to serializing operations]

tma_issueSmSt: [Metrics related by the issue $issueSmSt]
  tma_fb_full
       [This metric does a *rough estimation* of how often L1D Fill Buffer
        unavailability limited additional L1D miss memory access requests to
        proceed]

tma_issueSpSt: [Metrics related by the issue $issueSpSt]
  tma_port_4
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 4 (Store-data)]
  tma_split_stores
       [This metric represents rate of split store accesses]

tma_issueSyncxn: [Metrics related by the issue $issueSyncxn]
  tma_contested_accesses
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to contested accesses]
  tma_data_sharing
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to data-sharing accesses]
  tma_false_sharing
       [This metric roughly estimates how often CPU was handling
        synchronizations due to False Sharing]
  tma_machine_clears
       [This metric represents fraction of slots the CPU has wasted due to
        Machine Clears]

tma_issueTLB: [Metrics related by the issue $issueTLB]
  tma_dtlb_load
       [This metric roughly estimates the fraction of cycles where the Data
        TLB (DTLB) was missed by load accesses]
  tma_dtlb_store
       [This metric roughly estimates the fraction of cycles spent handling
        first-level data TLB store misses]
  tma_info_bottleneck_memory_data_tlbs
       [Total pipeline cost of Memory Address Translation related bottlenecks
        (data-side TLBs)]

tma_l1_bound_group: [Metrics contributing to tma_l1_bound category]
  tma_4k_aliasing
       [This metric estimates how often memory load accesses were aliased by
        preceding stores (in program order) with a 4K address offset]
  tma_dtlb_load
       [This metric roughly estimates the fraction of cycles where the Data
        TLB (DTLB) was missed by load accesses]
  tma_fb_full
       [This metric does a *rough estimation* of how often L1D Fill Buffer
        unavailability limited additional L1D miss memory access requests to
        proceed]
  tma_lock_latency
       [This metric represents fraction of cycles the CPU spent handling cache
        misses due to lock operations]
  tma_split_loads
       [This metric estimates fraction of cycles handling memory load split
        accesses - load that cross 64-byte cache line boundary]
  tma_store_fwd_blk
       [This metric roughly estimates fraction of cycles when the memory
        subsystem had loads blocked since they could not forward data from
        earlier (in program order) overlapping stores]

tma_l3_bound_group: [Metrics contributing to tma_l3_bound category]
  tma_contested_accesses
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to contested accesses]
  tma_data_sharing
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to data-sharing accesses]
  tma_l3_hit_latency
       [This metric represents fraction of cycles with demand load accesses
        that hit the L3 cache under unloaded scenarios (possibly L3 latency
        limited)]
  tma_sq_full
       [This metric measures fraction of cycles where the Super Queue (SQ) was
        full taking into account all request-types and both hardware SMT
        threads (Logical Processors)]

tma_light_operations_group: [Metrics contributing to tma_light_operations category]
  tma_fp_arith
       [This metric represents overall arithmetic floating-point (FP)
        operations fraction the CPU has executed (retired)]
  tma_fused_instructions
       [This metric represents fraction of slots where the CPU was retiring
        fused instructions -- where one uop can represent multiple contiguous
        instructions]
  tma_memory_operations
       [This metric represents fraction of slots where the CPU was retiring
        memory operations -- uops for memory load or store accesses]
  tma_non_fused_branches
       [This metric represents fraction of slots where the CPU was retiring
        branch instructions that were not fused]
  tma_nop_instructions
       [This metric represents fraction of slots where the CPU was retiring
        NOP (no op) instructions]
  tma_other_light_ops
       [This metric represents the remaining light uops fraction the CPU has
        executed - remaining means not covered by other sibling nodes]

tma_load_op_utilization_group: [Metrics contributing to tma_load_op_utilization category]
  tma_port_2
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 2 ([SNB+]Loads and Store-address; [ICL+] Loads)]
  tma_port_3
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 3 ([SNB+]Loads and Store-address; [ICL+] Loads)]

tma_memory_bound_group: [Metrics contributing to tma_memory_bound category]
  tma_dram_bound
       [This metric estimates how often the CPU was stalled on accesses to
        external memory (DRAM) by loads]
  tma_l1_bound
       [This metric estimates how often the CPU was stalled without loads
        missing the L1 data cache]
  tma_l2_bound
       [This metric estimates how often the CPU was stalled due to L2 cache
        accesses by loads]
  tma_l3_bound
       [This metric estimates how often the CPU was stalled due to loads
        accesses to L3 cache or contended with a sibling Core]
  tma_store_bound
       [This metric estimates how often CPU was stalled due to RFO store
        memory accesses; RFO store issue a read-for-ownership request before
        the write]

tma_microcode_sequencer_group: [Metrics contributing to tma_microcode_sequencer category]
  tma_assists
       [This metric estimates fraction of slots the CPU retired uops delivered
        by the Microcode_Sequencer as a result of Assists]
  tma_cisc
       [This metric estimates fraction of cycles the CPU retired uops
        originated from CISC (complex instruction set computer) instruction]

tma_mite_group: [Metrics contributing to tma_mite category]
  tma_decoder0_alone
       [This metric represents fraction of cycles where decoder-0 was the only
        active decoder]

tma_ports_utilization_group: [Metrics contributing to tma_ports_utilization category]
  tma_ports_utilized_0
       [This metric represents fraction of cycles CPU executed no uops on any
        execution port (Logical Processor cycles since ICL, Physical Core
        cycles otherwise)]
  tma_ports_utilized_1
       [This metric represents fraction of cycles where the CPU executed total
        of 1 uop per cycle on all execution ports (Logical Processor cycles
        since ICL, Physical Core cycles otherwise)]
  tma_ports_utilized_2
       [This metric represents fraction of cycles CPU executed total of 2 uops
        per cycle on all execution ports (Logical Processor cycles since ICL,
        Physical Core cycles otherwise)]
  tma_ports_utilized_3m
       [This metric represents fraction of cycles CPU executed total of 3 or
        more uops per cycle on all execution ports (Logical Processor cycles
        since ICL, Physical Core cycles otherwise)]

tma_ports_utilized_0_group: [Metrics contributing to tma_ports_utilized_0 category]
  tma_mixing_vectors
       [The Mixing_Vectors metric gives the percentage of injected blend uops
        out of all uops issued]
  tma_serializing_operation
       [This metric represents fraction of cycles the CPU issue-pipeline was
        stalled due to serializing operations]

tma_ports_utilized_3m_group: [Metrics contributing to tma_ports_utilized_3m category]
  tma_alu_op_utilization
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution ports for ALU operations]
  tma_load_op_utilization
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port for Load operations]
  tma_store_op_utilization
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port for Store operations]

tma_retiring_group: [Metrics contributing to tma_retiring category]
  tma_heavy_operations
       [This metric represents fraction of slots where the CPU was retiring
        heavy-weight operations -- instructions that require two or more uops
        or micro-coded sequences]
  tma_light_operations
       [This metric represents fraction of slots where the CPU was retiring
        light-weight operations -- instructions that require no more than one
        uop (micro-operation)]

tma_store_bound_group: [Metrics contributing to tma_store_bound category]
  tma_dtlb_store
       [This metric roughly estimates the fraction of cycles spent handling
        first-level data TLB store misses]
  tma_false_sharing
       [This metric roughly estimates how often CPU was handling
        synchronizations due to False Sharing]
  tma_split_stores
       [This metric represents rate of split store accesses]
  tma_store_latency
       [This metric estimates fraction of cycles the CPU spent handling L1D
        store misses]

tma_store_op_utilization_group: [Metrics contributing to tma_store_op_utilization category]
  tma_port_4
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 4 (Store-data)]
  tma_port_7
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 7 ([HSW+]simple Store-address)]

transaction:
  tsx_aborted_cycles
       [Percentage of cycles in aborted transactions]
  tsx_cycles_per_elision
       [Number of cycles within a transaction divided by the number of
        elisions]
  tsx_cycles_per_transaction
       [Number of cycles within a transaction divided by the number of
        transactions]
  tsx_transactional_cycles
       [Percentage of cycles within a transaction region]
